<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ankita Kushwaha, Jakku Niharika sri, Lingala Hasini">
<meta name="dcterms.date" content="2025-02-25">

<title>Skrub – Skrub blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-8e00f2f28a1266a720913de837032221.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7cfa380d139a4d61a924487b0af32b39.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Skrub blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Skrub</h1>
  <div class="quarto-categories">
    <div class="quarto-category">ML</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ankita Kushwaha, Jakku Niharika sri, Lingala Hasini </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 25, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><strong>SKRUB</strong> : A Python library for cleaning, structuring, and visualizing tabular data</p>
<p>INTRODUCTION:</p>
<p>How to deal with messy data Missing values, inconsistent formats, and unstructured information slow down data analysis? here skrub comes in picture! skrub is a powerful Python library designed to clean, structure, and prepare tabular data efficiently. In this guide, we’ll explore its key features</p>
<p>skrub makes cleaning, organizing, and visualizing messy tables easier and faster</p>
<p><strong>Data Cleaning</strong> – Removes inconsistencies, trims spaces, and fixes column names.</p>
<p><strong>Handling Missing Data</strong> – Easily fills missing values.</p>
<p><strong>Dataset Merging</strong>– Intelligently links datasets, even with slight variations.</p>
<p><strong>Quick Insights</strong>– Generates structured data for visualization &amp; analysis.</p>
<p><strong>WHY SKRUB?</strong></p>
<p><strong>Assembling Tables with Precision</strong>:Skrub excels at joining tables on keys of different types, including string, numerical, and datetime, with an impressive ability to handle imprecise correspondences.</p>
<p><strong>Fuzzy Joining for Seamless Integration</strong>: selects the type of fuzzy matching based on column types, producing a similarity score for easy identification of less-than-perfect matches</p>
<p><strong>Advanced Analysis Made Simple</strong>:Skrub takes table joining to the next level with features like Joiner, AggJoiner, and AggTarget.</p>
<p><strong>Efficient Column Selection in Pipelines</strong>:Apart from joins, skrub also facilitates column selection within a pipeline, allowing data scientists to choose and discard columns dynamically.</p>
<p><strong>INSTALLATION PROCESS</strong></p>
<div id="e5d262cf-e2f1-4b1d-a3e6-bf9334722ca2" class="cell" data-scrolled="true" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>pip install skrub</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: skrub in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (0.5.1)
Requirement already satisfied: numpy&gt;=1.23.5 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (2.2.1)
Requirement already satisfied: packaging&gt;=23.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (24.2)
Requirement already satisfied: pandas&gt;=1.5.3 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (2.2.3)
Requirement already satisfied: scikit-learn&gt;=1.2.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (1.6.1)
Requirement already satisfied: scipy&gt;=1.9.3 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (1.15.2)
Requirement already satisfied: jinja2&gt;=3.1.2 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (3.1.5)
Requirement already satisfied: matplotlib&gt;=3.4.3 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (3.10.0)
Requirement already satisfied: requests&gt;=2.25.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (2.32.3)
Requirement already satisfied: MarkupSafe&gt;=2.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from jinja2&gt;=3.1.2-&gt;skrub) (3.0.2)
Requirement already satisfied: contourpy&gt;=1.0.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (1.3.1)
Requirement already satisfied: cycler&gt;=0.10 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (0.12.1)
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (4.55.3)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (1.4.8)
Requirement already satisfied: pillow&gt;=8 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (11.1.0)
Requirement already satisfied: pyparsing&gt;=2.3.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (3.2.1)
Requirement already satisfied: python-dateutil&gt;=2.7 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from pandas&gt;=1.5.3-&gt;skrub) (2025.1)
Requirement already satisfied: tzdata&gt;=2022.7 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from pandas&gt;=1.5.3-&gt;skrub) (2025.1)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (2025.1.31)
Requirement already satisfied: joblib&gt;=1.2.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from scikit-learn&gt;=1.2.1-&gt;skrub) (1.4.2)
Requirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from scikit-learn&gt;=1.2.1-&gt;skrub) (3.5.0)
Requirement already satisfied: six&gt;=1.5 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.4.3-&gt;skrub) (1.17.0)
Note: you may need to restart the kernel to use updated packages.</code></pre>
</div>
</div>
<div id="566c3270-3b6b-4ee7-ae44-cb8393051c85" class="cell" data-scrolled="true" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>pip install <span class="op">--</span>upgrade skrub</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Requirement already satisfied: skrub in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (0.5.1)
Requirement already satisfied: numpy&gt;=1.23.5 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (2.2.1)
Requirement already satisfied: packaging&gt;=23.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (24.2)
Requirement already satisfied: pandas&gt;=1.5.3 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (2.2.3)
Requirement already satisfied: scikit-learn&gt;=1.2.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (1.6.1)
Requirement already satisfied: scipy&gt;=1.9.3 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (1.15.2)
Requirement already satisfied: jinja2&gt;=3.1.2 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (3.1.5)
Requirement already satisfied: matplotlib&gt;=3.4.3 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (3.10.0)
Requirement already satisfied: requests&gt;=2.25.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from skrub) (2.32.3)
Requirement already satisfied: MarkupSafe&gt;=2.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from jinja2&gt;=3.1.2-&gt;skrub) (3.0.2)
Requirement already satisfied: contourpy&gt;=1.0.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (1.3.1)
Requirement already satisfied: cycler&gt;=0.10 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (0.12.1)
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (4.55.3)
Requirement already satisfied: kiwisolver&gt;=1.3.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (1.4.8)
Requirement already satisfied: pillow&gt;=8 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (11.1.0)
Requirement already satisfied: pyparsing&gt;=2.3.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (3.2.1)
Requirement already satisfied: python-dateutil&gt;=2.7 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from matplotlib&gt;=3.4.3-&gt;skrub) (2.9.0.post0)
Requirement already satisfied: pytz&gt;=2020.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from pandas&gt;=1.5.3-&gt;skrub) (2025.1)
Requirement already satisfied: tzdata&gt;=2022.7 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from pandas&gt;=1.5.3-&gt;skrub) (2025.1)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (3.4.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (3.10)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (2.3.0)
Requirement already satisfied: certifi&gt;=2017.4.17 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from requests&gt;=2.25.0-&gt;skrub) (2025.1.31)
Requirement already satisfied: joblib&gt;=1.2.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from scikit-learn&gt;=1.2.1-&gt;skrub) (1.4.2)
Requirement already satisfied: threadpoolctl&gt;=3.1.0 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from scikit-learn&gt;=1.2.1-&gt;skrub) (3.5.0)
Requirement already satisfied: six&gt;=1.5 in c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.4.3-&gt;skrub) (1.17.0)
Note: you may need to restart the kernel to use updated packages.</code></pre>
</div>
</div>
<p><strong>USING SKRUB</strong></p>
<p><strong>HANDLING MISSING DATAFILES:</strong></p>
<p>We always find issues when cleaning data is dealing with missing or NaN values. With Skrub, you can fill or drop these missing values with just a few lines of code.</p>
<p>for example consider this code:</p>
<div id="5340ea90-7074-4a9a-a03b-98d4031e3453" class="cell" data-scrolled="true" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">#filling missing values</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Name'</span>: [<span class="st">'Alice'</span>, <span class="st">'Bob'</span>, <span class="va">None</span>, <span class="st">'Eve'</span>],</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Age'</span>: [<span class="dv">25</span>, <span class="va">None</span>, <span class="dv">22</span>, <span class="dv">29</span>],</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'City'</span>: [<span class="st">'New York'</span>, <span class="st">'Paris'</span>, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Name</th>
<th data-quarto-table-cell-role="th">Age</th>
<th data-quarto-table-cell-role="th">City</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Alice</td>
<td>25.0</td>
<td>New York</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Bob</td>
<td>NaN</td>
<td>Paris</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>None</td>
<td>22.0</td>
<td>None</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Eve</td>
<td>29.0</td>
<td>None</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>we can use skrub’s missing module to fill the missing values. For example, to fill numerical columns with the mean and categorical columns with the mode, we can use th following code:</p>
<div id="10875f00-cffc-4c81-a6a6-009965ab2197" class="cell" data-scrolled="true" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skrub <span class="im">import</span> TableVectorizer</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data with missing values</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> {</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Name'</span>: [<span class="st">'Alice'</span>, <span class="st">'Bob'</span>, <span class="va">None</span>, <span class="st">'Eve'</span>],</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Age'</span>: [<span class="dv">25</span>, <span class="va">None</span>, <span class="dv">22</span>, <span class="dv">29</span>],</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'City'</span>: [<span class="st">'New York'</span>, <span class="st">'Paris'</span>, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(data)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the original data</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original Data:"</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Handle missing values before using TableVectorizer</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's fill missing names with 'Unknown', ages with the mean of the column, and cities with 'Unknown'</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Name'</span>].fillna(<span class="st">'Unknown'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'Age'</span>].fillna(df[<span class="st">'Age'</span>].mean(), inplace<span class="op">=</span><span class="va">True</span>)  <span class="co"># Filling missing Age with the mean</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'City'</span>].fillna(<span class="st">'Unknown'</span>, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the data after filling missing values</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Data After Filling Missing Values:"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize TableVectorizer to clean and transform the data</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TableVectorizer()</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean the data (it will handle categorical data by encoding it into numerical form)</span></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>cleaned_data <span class="op">=</span> vectorizer.fit_transform(df)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert cleaned data to a DataFrame</span></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>df_cleaned <span class="op">=</span> pd.DataFrame(cleaned_data, columns<span class="op">=</span>vectorizer.get_feature_names_out())</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the cleaned data</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Cleaned Data (Transformed into Numerical Representation):"</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_cleaned)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Original Data:
    Name   Age      City
0  Alice  25.0  New York
1    Bob   NaN     Paris
2   None  22.0      None
3    Eve  29.0      None

Data After Filling Missing Values:
      Name        Age      City
0    Alice  25.000000  New York
1      Bob  25.333333     Paris
2  Unknown  22.000000   Unknown
3      Eve  29.000000   Unknown

Cleaned Data (Transformed into Numerical Representation):
   Name_Alice  Name_Bob  Name_Eve  Name_Unknown        Age  City_New York  \
0         1.0       0.0       0.0           0.0  25.000000            1.0   
1         0.0       1.0       0.0           0.0  25.333334            0.0   
2         0.0       0.0       0.0           1.0  22.000000            0.0   
3         0.0       0.0       1.0           0.0  29.000000            0.0   

   City_Paris  City_Unknown  
0         0.0           0.0  
1         1.0           0.0  
2         0.0           1.0  
3         0.0           1.0  </code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\nihar\AppData\Local\Temp\ipykernel_43672\3605782429.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df['Name'].fillna('Unknown', inplace=True)
C:\Users\nihar\AppData\Local\Temp\ipykernel_43672\3605782429.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df['Age'].fillna(df['Age'].mean(), inplace=True)  # Filling missing Age with the mean
C:\Users\nihar\AppData\Local\Temp\ipykernel_43672\3605782429.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df['City'].fillna('Unknown', inplace=True)</code></pre>
</div>
</div>
<p>The age column is replaced with its mean value.</p>
<p>the missing Name and City values are replaced with the string ‘Unknown’.</p>
<p>TableVectorizer handles the categorical features and converts them into binary features (one-hot encoding). Output The cleaned data is now numerical, and we can see how missing values were handled.</p>
<p><strong>Standardizing Data</strong></p>
<p>skrub can standardize numerical data, by making sure that values are consistent across the dataset. For example, if we have a column of Age values with a large range, we can scale it between 0 and 1 using normalization or standardization techniques.</p>
<p>for example analyze the code below</p>
<div id="08f5ff7e-4449-4e3b-8ffe-ad10a2fd52de" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skrub.preprocessing <span class="im">import</span> Normalizer</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">'Age'</span>: [<span class="dv">25</span>, <span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">40</span>, <span class="dv">45</span>]})</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalize data to a range [0, 1]</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>normalizer <span class="op">=</span> Normalizer()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>df_normalized <span class="op">=</span> normalizer.fit_transform(df)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_normalized)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg ansi-bold">ModuleNotFoundError</span>                       Traceback (most recent call last)
Cell <span class="ansi-green-fg ansi-bold">In[5], line 1</span>
<span class="ansi-green-fg ansi-bold">----&gt; 1</span> <span style="font-weight:bold;color:rgb(0,135,0)">from</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">skrub</span><span style="font-weight:bold;color:rgb(0,0,255)">.</span><span style="font-weight:bold;color:rgb(0,0,255)">preprocessing</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">import</span> Normalizer
<span class="ansi-green-fg">      2</span> <span style="font-weight:bold;color:rgb(0,135,0)">import</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">pandas</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,135,0)">as</span><span style="color:rgb(188,188,188)"> </span><span style="font-weight:bold;color:rgb(0,0,255)">pd</span>
<span class="ansi-green-fg">      4</span> <span style="font-style:italic;color:rgb(95,135,135)"># Sample data</span>

<span class="ansi-red-fg ansi-bold">ModuleNotFoundError</span>: No module named 'skrub.preprocessing'</pre>
</div>
</div>
</div>
<div id="b48121b0-b75e-4fd7-a952-765e670805d6" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> skrub</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">help</span>(skrub)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Help on package skrub:

NAME
    skrub - skrub: Prepping tables for machine learning.

PACKAGE CONTENTS
    _agg_joiner
    _check_dependencies
    _check_input
    _clean_categories
    _clean_null_strings
    _column_associations
    _dataframe (package)
    _datetime_encoder
    _deduplicate
    _dispatch
    _drop_if_too_many_nulls
    _fast_hash
    _fuzzy_join
    _gap_encoder
    _interpolation_joiner
    _join_utils
    _joiner
    _matching
    _minhash_encoder
    _multi_agg_joiner
    _on_each_column
    _on_subframe
    _reporting (package)
    _select_cols
    _selectors (package)
    _similarity_encoder
    _sklearn_compat
    _string_distances
    _string_encoder
    _table_vectorizer
    _tabular_learner
    _text_encoder
    _to_categorical
    _to_datetime
    _to_float32
    _to_str
    _utils
    _wrap_transformer
    conftest
    datasets (package)
    tests (package)

CLASSES
    builtins.object
        skrub._reporting._table_report.TableReport
    sklearn.base.BaseEstimator(sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin, sklearn.utils._metadata_requests._MetadataRequester)
        skrub._agg_joiner.AggJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._agg_joiner.AggTarget(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._interpolation_joiner.InterpolationJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._joiner.Joiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._multi_agg_joiner.MultiAggJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._select_cols.DropCols(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._select_cols.SelectCols(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._table_vectorizer.TableVectorizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
    sklearn.base.TransformerMixin(sklearn.utils._set_output._SetOutputMixin)
        skrub._agg_joiner.AggJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._agg_joiner.AggTarget(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._gap_encoder.GapEncoder(sklearn.base.TransformerMixin, skrub._on_each_column.SingleColumnTransformer)
        skrub._interpolation_joiner.InterpolationJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._joiner.Joiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._minhash_encoder.MinHashEncoder(sklearn.base.TransformerMixin, skrub._on_each_column.SingleColumnTransformer)
        skrub._multi_agg_joiner.MultiAggJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._select_cols.DropCols(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._select_cols.SelectCols(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._table_vectorizer.TableVectorizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
        skrub._text_encoder.TextEncoder(skrub._on_each_column.SingleColumnTransformer, sklearn.base.TransformerMixin)
    sklearn.preprocessing._encoders.OneHotEncoder(sklearn.preprocessing._encoders._BaseEncoder)
        skrub._similarity_encoder.SimilarityEncoder
    skrub._on_each_column.SingleColumnTransformer(sklearn.base.BaseEstimator)
        skrub._datetime_encoder.DatetimeEncoder
        skrub._gap_encoder.GapEncoder(sklearn.base.TransformerMixin, skrub._on_each_column.SingleColumnTransformer)
        skrub._minhash_encoder.MinHashEncoder(sklearn.base.TransformerMixin, skrub._on_each_column.SingleColumnTransformer)
        skrub._string_encoder.StringEncoder
        skrub._text_encoder.TextEncoder(skrub._on_each_column.SingleColumnTransformer, sklearn.base.TransformerMixin)
        skrub._to_categorical.ToCategorical
        skrub._to_datetime.ToDatetime

    class AggJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  AggJoiner(aux_table, operations, *, key=None, main_key=None, aux_key=None, cols=None, suffix='')
     |
     |  Aggregate an auxiliary dataframe before joining it on a base dataframe.
     |
     |  Apply numerical and categorical aggregation operations on the columns (i.e. `cols`)
     |  to aggregate. See the list of supported operations at the parameter `operations`.
     |
     |  If `cols` is not provided, `cols` are all columns from `aux_table`,
     |  except `aux_key`.
     |
     |  Accepts :obj:`pandas.DataFrame` and :class:`polars.DataFrame` inputs.
     |
     |  Parameters
     |  ----------
     |  aux_table : DataFrameLike or "X"
     |      Auxiliary dataframe to aggregate then join on the base table.
     |      The placeholder string "X" can be provided to perform
     |      self-aggregation on the input data.
     |
     |  operations : str or iterable of str
     |      Aggregation operations to perform on the auxiliary table.
     |
     |      Supported operations are "count", "mode", "min", "max", "sum", "median",
     |      "mean", "std". The operations "sum", "median", "mean", "std" are reserved
     |      to numeric type columns.
     |
     |  key : str, default=None
     |      The column name to use for both `main_key` and `aux_key` when they
     |      are the same. Provide either `key` or both `main_key` and `aux_key`.
     |      If `key` is an iterable, we will perform a multi-column join.
     |
     |  main_key : str or iterable of str, default=None
     |      Select the columns from the main table to use as keys during
     |      the join operation.
     |      If `main_key` is an iterable, we will perform a multi-column join.
     |
     |  aux_key : str or iterable of str, default=None
     |      Select the columns from the auxiliary dataframe to use as keys during
     |      the join operation.
     |      If `aux_key` is an iterable, we will perform a multi-column join.
     |
     |  cols : str or iterable of str, default=None
     |      Select the columns from the auxiliary dataframe to use as values during
     |      the aggregation operations.
     |      By default, `cols` are all columns from `aux_table`, except `aux_key`.
     |
     |  suffix : str, default=""
     |      Suffix to append to the `aux_table`'s column names. You can use it
     |      to avoid duplicate column names in the join.
     |
     |  See Also
     |  --------
     |  AggTarget :
     |      Aggregates the target `y` before joining its aggregation on the base dataframe.
     |
     |  Joiner :
     |      Augments a main table by automatically joining an auxiliary table on it.
     |
     |  MultiAggJoiner :
     |      Extension of the AggJoiner to multiple auxiliary tables.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import AggJoiner
     |  &gt;&gt;&gt; main = pd.DataFrame({
     |  ...     "airportId": [1, 2],
     |  ...     "airportName": ["Paris CDG", "NY JFK"],
     |  ... })
     |  &gt;&gt;&gt; aux = pd.DataFrame({
     |  ...     "flightId": range(1, 7),
     |  ...     "from_airport": [1, 1, 1, 2, 2, 2],
     |  ...     "total_passengers": [90, 120, 100, 70, 80, 90],
     |  ...     "company": ["DL", "AF", "AF", "DL", "DL", "TR"],
     |  ... })
     |  &gt;&gt;&gt; agg_joiner = AggJoiner(
     |  ...     aux_table=aux,
     |  ...     operations="mean",
     |  ...     main_key="airportId",
     |  ...     aux_key="from_airport",
     |  ...     cols="total_passengers",
     |  ... )
     |  &gt;&gt;&gt; agg_joiner.fit_transform(main)
     |     airportId  airportName  total_passengers_mean
     |  0          1    Paris CDG              103.33...
     |  1          2       NY JFK               80.00...
     |
     |  Method resolution order:
     |      AggJoiner
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, aux_table, operations, *, key=None, main_key=None, aux_key=None, cols=None, suffix='')
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Aggregate auxiliary table based on the main keys.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          Input data, based table on which to left join the
     |          auxiliary table.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      AggJoiner
     |          Fitted :class:`AggJoiner` instance (self).
     |
     |  fit_transform(self, X, y=None)
     |      Aggregate auxiliary table based on the main keys.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          Input data, based table on which to left join the
     |          auxiliary table.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The augmented input.
     |
     |  get_feature_names_out(self)
     |      Get output feature names for transformation.
     |
     |      Returns
     |      -------
     |      List of str
     |          Transformed feature names.
     |
     |  transform(self, X)
     |      Left-join pre-aggregated table on `X`.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          The input data to transform.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The augmented input.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class AggTarget(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  AggTarget(main_key, operations, *, suffix='_target')
     |
     |  Aggregate a target `y` before joining its aggregation on a base dataframe.
     |
     |  Accepts :obj:`pandas.DataFrame` or :class:`polars.DataFrame` inputs.
     |
     |  Parameters
     |  ----------
     |  main_key : str or iterable of str
     |      Select the columns from the main table to use as keys during
     |      the aggregation of the target and during the join operation.
     |
     |      If `main_key` refer to a single column, a single aggregation
     |      for this key will be generated and a single join will be performed.
     |
     |      If `main_key` is a list of keys, a multi-column aggregation will be performed
     |      on the target.
     |
     |  operations : str or iterable of str
     |      Aggregation operations to perform on the target.
     |
     |      Supported operations are "count", "mode", "min", "max", "sum", "median",
     |      "mean", "std". The operations "sum", "median", "mean", "std" are reserved
     |      to numeric type targets.
     |
     |  suffix : str, default="_target"
     |      The suffix to append to the columns of the target table if the join
     |      results in duplicates columns.
     |
     |  See Also
     |  --------
     |  AggJoiner :
     |      Aggregates auxiliary dataframes before joining them
     |      on the base dataframe.
     |
     |  Joiner :
     |      Augments a main table by automatically joining multiple
     |      auxiliary tables on it.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; import numpy as np
     |  &gt;&gt;&gt; from skrub import AggTarget
     |  &gt;&gt;&gt; X = pd.DataFrame({
     |  ...     "flightId": range(1, 7),
     |  ...     "from_airport": [1, 1, 1, 2, 2, 2],
     |  ...     "total_passengers": [90, 120, 100, 70, 80, 90],
     |  ...     "company": ["DL", "AF", "AF", "DL", "DL", "TR"],
     |  ... })
     |  &gt;&gt;&gt; y = np.array([1, 1, 0, 0, 1, 1])
     |  &gt;&gt;&gt; agg_target = AggTarget(
     |  ...     main_key="company",
     |  ...     operations=["mean", "max"],
     |  ... )
     |  &gt;&gt;&gt; agg_target.fit_transform(X, y)
     |     flightId  from_airport  ...  y_0_mean_target  y_0_max_target
     |  0         1             1  ...         0.666667               1
     |  1         2             1  ...         0.500000               1
     |  2         3             1  ...         0.500000               1
     |  3         4             2  ...         0.666667               1
     |  4         5             2  ...         0.666667               1
     |  5         6             2  ...         1.000000               1
     |
     |  Method resolution order:
     |      AggTarget
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, main_key, operations, *, suffix='_target')
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y)
     |      Aggregate the target `y` based on keys from `X`.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          Must contains the columns names defined in `main_key`.
     |
     |      y : DataFrameLike or SeriesLike or ArrayLike
     |          `y` length must match `X` length.
     |          The target can be continuous or discrete, with multiple columns.
     |
     |      Returns
     |      -------
     |      AggTarget
     |          Fitted :class:`AggTarget` instance (self).
     |
     |  fit_transform(self, X, y)
     |      Aggregate the target `y` based on keys from `X`.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          Must contains the columns names defined in `main_key`.
     |
     |      y : DataFrameLike or SeriesLike or ArrayLike
     |          `y` length must match `X` length.
     |          The target can be continuous or discrete, with multiple columns.
     |
     |      Returns
     |      -------
     |      Dataframe
     |          The augmented input.
     |
     |  get_feature_names_out(self)
     |      Get output feature names for transformation.
     |
     |      Returns
     |      -------
     |      List of str
     |          Transformed feature names.
     |
     |  transform(self, X)
     |      Left-join pre-aggregated target on `X`.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          The input data to transform.
     |
     |      Returns
     |      -------
     |      X_transformed : DataFrameLike
     |          The augmented input.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class DatetimeEncoder(skrub._on_each_column.SingleColumnTransformer)
     |  DatetimeEncoder(resolution='hour', add_weekday=False, add_total_seconds=True)
     |
     |  Extract temporal features such as month, day of the week, … from a datetime column.
     |
     |  .. note::
     |
     |      ``DatetimeEncoder`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((DatetimeEncoder(), 'col_name_1'),
     |      (DatetimeEncoder(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((DatetimeEncoder(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  All extracted features are provided as float32 columns.
     |
     |  No timezone conversion is performed: if the input column is timezone aware, the
     |  extracted features will be in the column's timezone.
     |
     |  An input column that does not have a Date or Datetime dtype will be
     |  rejected by raising a ``RejectColumn`` exception. See ``ToDatetime`` for
     |  converting strings to proper datetimes. **Note:** the ``TableVectorizer``
     |  only sends datetime columns to its ``datetime_encoder``. Therefore it is
     |  always safe to use a ``DatetimeEncoder`` as the ``TableVectorizer``'s
     |  ``datetime_encoder`` parameter.
     |
     |  Parameters
     |  ----------
     |  resolution : str or None, default="hour"
     |      If a string, extract up to this resolution. Must be "year", "month",
     |      "day", "hour", "minute", "second", "microsecond", or "nanosecond". For
     |      example, ``resolution="day"`` generates the features "year", "month",
     |      and "day" only. If the input column contains dates with no time
     |      information, time features ("hour", "minute", … ) are never extracted.
     |      If ``None``, the features listed above are not extracted (but day of
     |      the week and total seconds may still be extracted, see below).
     |
     |  add_weekday : bool, default=False
     |      Extract the day of the week as a numerical feature from 1 (Monday) to 7
     |      (Sunday).
     |
     |  add_total_seconds : bool, default=True
     |      Add the total number of seconds since the Unix epoch (00:00:00 UTC on 1
     |      January 1970).
     |
     |  Attributes
     |  ----------
     |  extracted_features_ : list of strings
     |      The features that are extracted, a subset of ["year", …, "nanosecond",
     |      "weekday", "total_seconds"]
     |
     |  See Also
     |  --------
     |  ToDatetime :
     |      Convert strings to datetimes.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |
     |  &gt;&gt;&gt; login = pd.to_datetime(
     |  ...     pd.Series(
     |  ...         ["2024-05-13T12:05:36", None, "2024-05-15T13:46:02"], name="login")
     |  ... )
     |  &gt;&gt;&gt; login
     |  0   2024-05-13 12:05:36
     |  1                   NaT
     |  2   2024-05-15 13:46:02
     |  Name: login, dtype: datetime64[...]
     |  &gt;&gt;&gt; from skrub import DatetimeEncoder
     |
     |  &gt;&gt;&gt; DatetimeEncoder().fit_transform(login)
     |     login_year  login_month  login_day  login_hour  login_total_seconds
     |  0      2024.0          5.0       13.0        12.0         1.715602e+09
     |  1         NaN          NaN        NaN         NaN                  NaN
     |  2      2024.0          5.0       15.0        13.0         1.715781e+09
     |
     |  We can ask for a finer resolution:
     |
     |  &gt;&gt;&gt; DatetimeEncoder(resolution='second', add_total_seconds=False).fit_transform(
     |  ...     login
     |  ... )
     |     login_year  login_month  login_day  login_hour  login_minute  login_second
     |  0      2024.0          5.0       13.0        12.0           5.0          36.0
     |  1         NaN          NaN        NaN         NaN           NaN           NaN
     |  2      2024.0          5.0       15.0        13.0          46.0           2.0
     |
     |  We can also ask for the day of the week. The week starts at 1 on Monday and ends
     |  at 7 on Sunday. This is consistent with the ISO week date system
     |  (https://en.wikipedia.org/wiki/ISO_week_date), the standard library
     |  ``datetime.isoweekday()`` and polars ``weekday``, but not with pandas
     |  ``day_of_week``, which counts days from 0.
     |
     |  &gt;&gt;&gt; login.dt.strftime('%A = %w')
     |  0       Monday = 1
     |  1              NaN
     |  2    Wednesday = 3
     |  Name: login, dtype: object
     |  &gt;&gt;&gt; login.dt.day_of_week
     |  0    0.0
     |  1    NaN
     |  2    2.0
     |  Name: login, dtype: float64
     |  &gt;&gt;&gt; DatetimeEncoder(add_weekday=True, add_total_seconds=False).fit_transform(login)
     |     login_year  login_month  login_day  login_hour  login_weekday
     |  0      2024.0          5.0       13.0        12.0            1.0
     |  1         NaN          NaN        NaN         NaN            NaN
     |  2      2024.0          5.0       15.0        13.0            3.0
     |
     |  When a column contains only dates without time information, the time features
     |  are discarded, regardless of ``resolution``.
     |
     |  &gt;&gt;&gt; birthday = pd.to_datetime(
     |  ...     pd.Series(['2024-04-14', '2024-05-15'], name='birthday')
     |  ... )
     |  &gt;&gt;&gt; encoder = DatetimeEncoder(resolution='second')
     |  &gt;&gt;&gt; encoder.fit_transform(birthday)
     |     birthday_year  birthday_month  birthday_day  birthday_total_seconds
     |  0         2024.0             4.0          14.0            1.713053e+09
     |  1         2024.0             5.0          15.0            1.715731e+09
     |  &gt;&gt;&gt; encoder.extracted_features_
     |  ['year', 'month', 'day', 'total_seconds']
     |
     |  (The number of seconds since Epoch can still be extracted but not "hour",
     |  "minute", etc.)
     |
     |  Non-datetime columns are rejected by raising a ``RejectColumn`` exception.
     |
     |  &gt;&gt;&gt; s = pd.Series(['2024-04-14', '2024-05-15'], name='birthday')
     |  &gt;&gt;&gt; s
     |  0    2024-04-14
     |  1    2024-05-15
     |  Name: birthday, dtype: object
     |  &gt;&gt;&gt; DatetimeEncoder().fit_transform(s)
     |  Traceback (most recent call last):
     |      ...
     |  skrub._on_each_column.RejectColumn: Column 'birthday' does not have Date or Datetime dtype.
     |
     |  :class:`ToDatetime`: can be used for converting strings to datetimes.
     |
     |  &gt;&gt;&gt; from skrub import ToDatetime
     |  &gt;&gt;&gt; from sklearn.pipeline import make_pipeline
     |  &gt;&gt;&gt; make_pipeline(ToDatetime(), DatetimeEncoder()).fit_transform(s)
     |     birthday_year  birthday_month  birthday_day  birthday_total_seconds
     |  0         2024.0             4.0          14.0            1.713053e+09
     |  1         2024.0             5.0          15.0            1.715731e+09
     |
     |  **Time zones**
     |
     |  If the input column has a time zone, the extracted features are in this timezone.
     |
     |  &gt;&gt;&gt; login = pd.to_datetime(
     |  ...     pd.Series(
     |  ...         ["2024-05-13T12:05:36", None, "2024-05-15T13:46:02"], name="login")
     |  ... ).dt.tz_localize('Europe/Paris')
     |  &gt;&gt;&gt; encoder = DatetimeEncoder()
     |  &gt;&gt;&gt; encoder.fit_transform(login)['login_hour']
     |  0    12.0
     |  1     NaN
     |  2    13.0
     |  Name: login_hour, dtype: float32
     |
     |  No special care is taken to convert inputs to ``transform`` to the same time
     |  zone as the column the encoder was fitted on. The features are always in the
     |  time zone of the input.
     |
     |  &gt;&gt;&gt; login_sp = login.dt.tz_convert('America/Sao_Paulo')
     |  &gt;&gt;&gt; login_sp
     |  0   2024-05-13 07:05:36-03:00
     |  1                         NaT
     |  2   2024-05-15 08:46:02-03:00
     |  Name: login, dtype: datetime64[..., America/Sao_Paulo]
     |  &gt;&gt;&gt; encoder.transform(login_sp)['login_hour']
     |  0    7.0
     |  1    NaN
     |  2    8.0
     |  Name: login_hour, dtype: float32
     |
     |  To ensure datetime columns are in a consistent timezones, use ``ToDatetime``.
     |
     |  &gt;&gt;&gt; encoder = make_pipeline(ToDatetime(), DatetimeEncoder())
     |  &gt;&gt;&gt; encoder.fit_transform(login)['login_hour']
     |  0    12.0
     |  1     NaN
     |  2    13.0
     |  Name: login_hour, dtype: float32
     |  &gt;&gt;&gt; encoder.transform(login_sp)['login_hour']
     |  0    12.0
     |  1     NaN
     |  2    13.0
     |  Name: login_hour, dtype: float32
     |
     |  Here we can see the input to ``transform`` has been converted back to the
     |  timezone used during ``fit`` and that we get the same result for "hour".
     |
     |  Method resolution order:
     |      DatetimeEncoder
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, resolution='hour', add_weekday=False, add_total_seconds=True)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __sklearn_tags__(self)
     |
     |  fit_transform(self, column, y=None)
     |      Fit the encoder and transform a column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series with dtype Date or Datetime
     |          The input to transform.
     |
     |      y : None
     |          Ignored.
     |
     |      Returns
     |      -------
     |      transformed : DataFrame
     |          The extracted features.
     |
     |  set_fit_request(self: skrub._datetime_encoder.DatetimeEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._datetime_encoder.DatetimeEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  set_transform_request(self: skrub._datetime_encoder.DatetimeEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._datetime_encoder.DatetimeEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``transform`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``transform``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  transform(self, column)
     |      Transform a column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series with dtype Date or Datetime
     |          The input to transform.
     |
     |      Returns
     |      -------
     |      transformed : DataFrame
     |          The extracted features.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  fit(self, column, y=None)
     |      Fit the transformer.
     |
     |      Subclasses should implement ``fit_transform`` and ``transform``.
     |
     |      Parameters
     |      ----------
     |      column : a pandas or polars Series
     |          Unlike most scikit-learn transformers, single-column transformers
     |          transform a single column, not a whole dataframe.
     |
     |      y : column or dataframe
     |          Prediction targets.
     |
     |      Returns
     |      -------
     |      self
     |          The fitted transformer.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __init_subclass__(**kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class DropCols(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  DropCols(cols)
     |
     |  Drop a subset of a DataFrame's columns.
     |
     |  The other columns are kept in their original order. A ``ValueError`` is
     |  raised if any of the provided column names are not in the dataframe.
     |
     |  Accepts :obj:`pandas.DataFrame` and :obj:`polars.DataFrame` inputs.
     |
     |  Parameters
     |  ----------
     |  cols : list of str or str
     |      The columns to drop. A single column name can be passed as a ``str``:
     |      ``"col_name"`` is the same as ``["col_name"]``.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import DropCols
     |  &gt;&gt;&gt; df = pd.DataFrame({"A": [1, 2], "B": [10, 20], "C": ["x", "y"]})
     |  &gt;&gt;&gt; df
     |     A   B  C
     |  0  1  10  x
     |  1  2  20  y
     |  &gt;&gt;&gt; DropCols(["A", "C"]).fit_transform(df)
     |      B
     |  0  10
     |  1  20
     |  &gt;&gt;&gt; DropCols(["X"]).fit_transform(df)
     |  Traceback (most recent call last):
     |      ...
     |  ValueError: The following columns are requested for selection but missing from dataframe: ['X']
     |
     |  Method resolution order:
     |      DropCols
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, cols)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Fit the transformer.
     |
     |      Parameters
     |      ----------
     |      X : DataFrame or None
     |          If `X` is a DataFrame, the transformer checks that all the column
     |          names provided in ``self.cols`` can be found in `X`.
     |
     |      y : None
     |          Unused.
     |
     |      Returns
     |      -------
     |      DropCols
     |          The transformer itself.
     |
     |  transform(self, X)
     |      Transform a dataframe by dropping columns.
     |
     |      Parameters
     |      ----------
     |      X : DataFrame
     |          The DataFrame on which to apply the selection.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The input DataFrame ``X`` after dropping the columns listed in
     |          ``self.cols``.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |
     |      Fits transformer to `X` and `y` with optional parameters `fit_params`
     |      and returns a transformed version of `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input samples.
     |
     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None
     |          Target values (None for unsupervised transformations).
     |
     |      **fit_params : dict
     |          Additional fit parameters.
     |
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class GapEncoder(sklearn.base.TransformerMixin, skrub._on_each_column.SingleColumnTransformer)
     |  GapEncoder(n_components=10, batch_size=1024, gamma_shape_prior=1.1, gamma_scale_prior=1.0, rho=0.95, rescale_rho=False, hashing=False, hashing_n_features=4096, init='k-means++', max_iter=5, ngram_range=(2, 4), analyzer='char', add_words=False, random_state=None, rescale_W=True, max_iter_e_step=1, max_no_improvement=5, verbose=0)
     |
     |  Encode string columns by constructing latent topics.
     |
     |  .. note::
     |
     |      ``GapEncoder`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((GapEncoder(), 'col_name_1'),
     |      (GapEncoder(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((GapEncoder(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  This encoder can be understood as a continuous encoding on a set of latent
     |  categories estimated from the data. The latent categories are built by
     |  capturing combinations of substrings that frequently co-occur.
     |
     |  The GapEncoder supports online learning on batches of
     |  data for scalability through the GapEncoder.partial_fit
     |  method.
     |
     |  The principle is as follows:
     |
     |  1. Given an input string array `X`, we build its bag-of-n-grams
     |     representation `V` (`n_samples`, `vocab_size`).
     |  2. Instead of using the n-grams counts as encodings, we look for low-
     |     dimensional representations by modeling n-grams counts as linear
     |     combinations of topics ``V = HW``, with `W` (`n_topics`, `vocab_size`)
     |     the topics and `H` (`n_samples`, `n_topics`) the associated activations.
     |  3. Assuming that n-grams counts follow a Poisson law, we fit `H` and `W` to
     |     maximize the likelihood of the data, with a Gamma prior for the
     |     activations `H` to induce sparsity.
     |  4. In practice, this is equivalent to a non-negative matrix factorization
     |     with the Kullback-Leibler divergence as loss, and a Gamma prior on `H`.
     |     We thus optimize `H` and `W` with the multiplicative update method.
     |
     |  "Gap" stands for "Gamma-Poisson", the families of distributions that are
     |  used to model the importance of topics in a document (Gamma), and the term
     |  frequencies in a document (Poisson).
     |
     |  Input columns that do not have a string or Categorical dtype are rejected
     |  by raising a ``RejectColumn`` exception.
     |
     |  Parameters
     |  ----------
     |  n_components : int, optional, default=10
     |      Number of latent categories used to model string data.
     |  batch_size : int, optional, default=1024
     |      Number of samples per batch.
     |  gamma_shape_prior : float, optional, default=1.1
     |      Shape parameter for the Gamma prior distribution.
     |  gamma_scale_prior : float, optional, default=1.0
     |      Scale parameter for the Gamma prior distribution.
     |  rho : float, optional, default=0.95
     |      Weight parameter for the update of the `W` matrix.
     |  rescale_rho : bool, optional, default=False
     |      If `True`, use ``rho ** (batch_size / len(X))`` instead of rho to obtain
     |      an update rate per iteration that is independent of the batch size.
     |  hashing : bool, optional, default=False
     |      If `True`, HashingVectorizer is used instead of CountVectorizer.
     |      It has the advantage of being very low memory, scalable to large
     |      datasets as there is no need to store a vocabulary dictionary in
     |      memory.
     |  hashing_n_features : int, default=2**12
     |      Number of features for the HashingVectorizer.
     |      Only relevant if `hashing=True`.
     |  init : {'k-means++', 'random', 'k-means'}, default='k-means++'
     |      Initialization method of the `W` matrix.
     |      If `init='k-means++'`, we use the init method of KMeans.
     |      If `init='random'`, topics are initialized with a Gamma distribution.
     |      If `init='k-means'`, topics are initialized with a KMeans on the
     |      n-grams counts.
     |  max_iter : int, default=5
     |      Maximum number of iterations on the input data.
     |  ngram_range : int 2-tuple, default=(2, 4)
     |     The lower and upper boundaries of the range of n-values for different
     |      n-grams used in the string similarity. All values of `n` such
     |      that ``min_n &lt;= n &lt;= max_n`` will be used.
     |  analyzer : {'word', 'char', 'char_wb'}, default='char'
     |      Analyzer parameter for the HashingVectorizer / CountVectorizer.
     |      Describes whether the matrix `V` to factorize should be made of
     |      word counts or character-level n-gram counts.
     |      Option ‘char_wb’ creates character n-grams only from text inside word
     |      boundaries; n-grams at the edges of words are padded with space.
     |  add_words : bool, default=False
     |      If `True`, add the words counts to the bag-of-n-grams representation
     |      of the input data.
     |  random_state : int or RandomState, optional
     |      Random number generator seed for reproducible output across multiple
     |      function calls.
     |  rescale_W : bool, default=True
     |      If `True`, the weight matrix `W` is rescaled at each iteration
     |      to have a l1 norm equal to 1 for each row.
     |  max_iter_e_step : int, default=1
     |      Maximum number of iterations to adjust the activations h at each step.
     |  max_no_improvement : int, default=5
     |      Control early stopping based on the consecutive number of mini batches
     |      that do not yield an improvement on the smoothed cost function.
     |      To disable early stopping and run the process fully,
     |      set ``max_no_improvement=None``.
     |  verbose : int, default=0
     |      Verbosity level. The higher, the more granular the logging.
     |
     |  Attributes
     |  ----------
     |  rho_ : float
     |      Effective update rate for the `W` matrix.
     |  fitted_models_ : list of GapEncoderColumn
     |      Column-wise fitted GapEncoders.
     |  column_names_ : list of str
     |      Column names of the data the Gap was fitted on.
     |
     |  See Also
     |  --------
     |  MinHashEncoder :
     |      Encode string columns as a numeric array with the minhash method.
     |  SimilarityEncoder :
     |      Encode string columns as a numeric array with n-gram string similarity.
     |  TextEncoder :
     |      Encode string columns with a pretrained language model.
     |  StringEncoder
     |      Fast n-gram encoding of string columns.
     |  deduplicate :
     |      Deduplicate data by hierarchically clustering similar strings.
     |
     |  References
     |  ----------
     |  For a detailed description of the method, see
     |  `Encoding high-cardinality string categorical variables
     |  &lt;https://hal.inria.fr/hal-02171256v4&gt;`_ by Cerda, Varoquaux (2019).
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import GapEncoder
     |  &gt;&gt;&gt; enc = GapEncoder(n_components=2, random_state=0)
     |
     |  Let's encode the following non-normalized data:
     |
     |  &gt;&gt;&gt; X = pd.Series(['Paris, FR', 'Paris', 'London, UK', 'Paris, France',
     |  ...                'london', 'London, England', 'London', 'Pqris'], name='city')
     |  &gt;&gt;&gt; enc.fit(X)
     |  GapEncoder(n_components=2, random_state=0)
     |
     |  The GapEncoder has found the following two topics:
     |
     |  &gt;&gt;&gt; enc.get_feature_names_out()
     |  ['city: england, london, uk', 'city: france, paris, pqris']
     |
     |  It got it right, reoccurring topics are "London" and "England" on the
     |  one side and "Paris" and "France" on the other.
     |
     |  As this is a continuous encoding, we can look at the level of
     |  activation of each topic for each category:
     |
     |  &gt;&gt;&gt; enc.transform(X)
     |     city: england, london, uk  city: france, paris, pqris
     |  0                   0.051816                   10.548184
     |  1                   0.050134                    4.549866
     |  2                  12.046517                    0.053483
     |  3                   0.052270                   16.547730
     |  4                   6.049970                    0.050030
     |  5                  19.545227                    0.054773
     |  6                   6.049970                    0.050030
     |  7                   0.060120                    4.539880
     |
     |  The higher the value, the bigger the correspondence with the topic.
     |
     |  Method resolution order:
     |      GapEncoder
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, n_components=10, batch_size=1024, gamma_shape_prior=1.1, gamma_scale_prior=1.0, rho=0.95, rescale_rho=False, hashing=False, hashing_n_features=4096, init='k-means++', max_iter=5, ngram_range=(2, 4), analyzer='char', add_words=False, random_state=None, rescale_W=True, max_iter_e_step=1, max_no_improvement=5, verbose=0)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Fit the GapEncoder on `X`.
     |
     |      Parameters
     |      ----------
     |      X : Column, shape (n_samples, )
     |          The string data to fit the model on.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      GapEncoderColumn
     |          The fitted GapEncoderColumn instance (self).
     |
     |  get_feature_names_out(self, n_labels=3)
     |      Return the labels that best summarize the learned components/topics.
     |
     |      For each topic, labels with the highest activations are selected.
     |
     |      Parameters
     |      ----------
     |      n_labels : int, default=3
     |          The number of labels used to describe each topic.
     |
     |      Returns
     |      -------
     |      list of str
     |          The labels that best describe each topic.
     |
     |  partial_fit(self, X, y=None)
     |      Partial fit this instance on `X`.
     |
     |      To be used in an online learning procedure where batches of data are
     |      coming one by one.
     |
     |      Parameters
     |      ----------
     |      X : Column, shape (n_samples, )
     |          The string data to fit the model on.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      GapEncoderColumn
     |          The fitted GapEncoderColumn instance (self).
     |
     |  score(self, X)
     |      Score this instance of `X`.
     |
     |      Returns the Kullback-Leibler divergence between the n-grams counts
     |      matrix `V` of `X`, and its non-negative factorization `HW`.
     |
     |      Parameters
     |      ----------
     |      X : Column, shape (n_samples, )
     |          The data to encode.
     |
     |      Returns
     |      -------
     |      float
     |          The Kullback-Leibler divergence.
     |
     |  transform(self, X)
     |      Return the encoded vectors (activations) `H` of input strings in `X`.
     |
     |      Parameters
     |      ----------
     |      X : Column, shape (n_samples)
     |          The string data to encode.
     |
     |      Returns
     |      -------
     |      DataFrame, shape (n_samples, n_topics)
     |          Transformed input.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |
     |      Fits transformer to `X` and `y` with optional parameters `fit_params`
     |      and returns a transformed version of `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input samples.
     |
     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None
     |          Target values (None for unsupervised transformations).
     |
     |      **fit_params : dict
     |          Additional fit parameters.
     |
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  set_fit_request(self: skrub._gap_encoder.GapEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._gap_encoder.GapEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class InterpolationJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  InterpolationJoiner(aux_table, *, key=None, main_key=None, aux_key=None, suffix='', regressor=HistGradientBoostingRegressor(), classifier=HistGradientBoostingClassifier(), vectorizer=TableVectorizer(high_cardinality=MinHashEncoder()), n_jobs=None, on_estimator_failure='warn')
     |
     |  Join with a table augmented by machine-learning predictions.
     |
     |  This is similar to a usual equi-join, but instead of looking for actual
     |  rows in the right table that satisfy the join condition, we estimate what
     |  those rows would contain if they existed in the table.
     |
     |  Suppose we want to join a table ``buildings(latitude, longitude, n_stories)``
     |  with a table ``annual_avg_temp(latitude, longitude, avg_temp)``. Our annual
     |  average temperature table may not contain data for the exact latitude and
     |  longitude of our buildings. However, we can interpolate what we need from
     |  the data points it does contain. Using ``annual_avg_temp``, we train a
     |  model to predict the temperature, given the latitude and longitude. Then,
     |  we use this model to estimate the values we want to add to our
     |  ``buildings`` table. In a way we are joining ``buildings`` to a virtual
     |  table, in which rows for any (latitude, longitude) location are inferred,
     |  rather than retrieved, when requested. This is done with::
     |
     |      InterpolationJoiner(
     |          annual_avg_temp, on=["latitude", "longitude"]
     |      ).fit_transform(buildings)
     |
     |  Parameters
     |  ----------
     |  aux_table : DataFrame
     |      The (auxiliary) table to be joined to the ``main_table`` (which is the
     |      argument of ``transform``). ``aux_table`` is used to train a model that
     |      takes as inputs the contents of the columns listed in ``aux_key``, and
     |      predicts the contents of the other columns. In the example above, we
     |      want our transformer to add temperature data to the table it is
     |      operating on. Therefore, ``aux_table`` is the ``annual_avg_temp``
     |      table.
     |
     |  key : str or list of str, default=None
     |      Column names to use for both ``main_key`` and ``aux_key``, when they are
     |      the same. Provide either ``key`` (only) or both ``main_key`` and ``aux_key``.
     |
     |  main_key : str or list of str, default=None
     |      The columns in the main table used for joining. The main table is the
     |      argument of ``transform``, to which we add information inferred using
     |      ``aux_table``. The column names listed in ``main_key`` will provide the
     |      inputs (features) of the interpolators at prediction (joining) time. In
     |      the example above, ``main_key`` is ``["latitude", "longitude"]``, which
     |      refer to columns in the ``buildings`` table. When joining on a single
     |      column, we can pass its name rather than a list: ``"latitude"`` is
     |      equivalent to ``["latitude"]``.
     |
     |  aux_key : str or list of str, default=None
     |      The columns in ``aux_table`` used for joining. Their number and types
     |      must match those of the ``main_key`` columns in the main table. These
     |      columns provide the features for the estimators to be fitted. As for
     |      ``main_key``, it is possible to pass a string when using a single
     |      column.
     |
     |  suffix : str, default=""
     |      Suffix to append to the ``aux_table``'s column names. If duplicate column
     |      names are found, a __skrub_&lt;random string&gt;__ is added at the end of columns
     |      that would otherwise be duplicates.
     |
     |  regressor : scikit-learn regressor, default=HistGradientBoostingRegressor
     |      Model used to predict the numerical columns of ``aux_table``.
     |
     |  classifier : scikit-learn classifier, default=HistGradientBoostingClassifier
     |      Model used to predict the string and categorical columns of ``aux_table``.
     |
     |  vectorizer : scikit-learn transformer that can operate on a DataFrame
     |      Used to transform the feature columns before passing them to the
     |      scikit-learn estimators. This is useful if we are joining on columns
     |      that need some transformation, such as dates or strings representing
     |      high-cardinality categories. By default we use a ``MinHashEncoder`` to
     |      vectorize text columns. This is because the ``MinHashEncoder`` is very
     |      fast and usually gives good results with downstream learners based on
     |      trees like the gradient-boosted trees used by default for ``regressor``
     |      and ``classifier``. If you replace the default regressor and classifier
     |      with models such as nearest-neighbors or linear models, consider
     |      passing ``vectorizer=TableVectorizer()`` which will encode text with a
     |      ``GapEncoder`` rather than a ``MinHashEncoder``.
     |
     |  n_jobs : int or None, default=None
     |      Number of jobs to run in parallel. ``None`` means 1 unless in a
     |      ``joblib.parallel_backend`` context. -1 means using all processors.
     |      Depending on the estimators used and the contents of ``aux_table``,
     |      several estimators may need to be fitted -- for example one for
     |      continuous outputs (regressor) and one for categorical outputs
     |      (classifier), or one for each column when the provided estimators do
     |      not support multi-output tasks. Fitting and querying these estimators
     |      can be done in parallel.
     |
     |  on_estimator_failure : "warn", "raise" or "pass", default="warn"
     |      How to handle exceptions raised when fitting one of the estimators
     |      (regressors and classifiers) or querying them for a prediction. If
     |      "raise", exceptions are propagated. If "pass" (i) if an exception is
     |      raised during ``fit`` the corresponding columns are ignored -- they
     |      will not appear in the join and (ii) if an exception is raised during
     |      ``transform``, the corresponding column will be filled with nulls.
     |      Columns are filled with nulls during ``transform`` rather than dropped
     |      so that the output always has the same shape. If "warn" (the default),
     |      behave like "pass" but issue a warning.
     |
     |  Attributes
     |  ----------
     |  vectorizer_ : scikit-learn transformer
     |      The transformer used to vectorize the feature columns.
     |
     |  estimators_ : list of dicts
     |      The estimators used to infer values to be joined. Each entry in this
     |      list is a dictionary with keys ``"estimator"`` (the fitted estimator)
     |      and ``"columns"`` (the list of columns in ``aux_table`` that it is
     |      trained to predict).
     |
     |  See Also
     |  --------
     |  Joiner :
     |      Works in a similar way but instead of inferring values, picks the
     |      closest row from the auxiliary table.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; buildings = pd.DataFrame(
     |  ...     {"latitude": [1.0, 2.0], "longitude": [1.0, 2.0], "n_stories": [3, 7]}
     |  ... )
     |  &gt;&gt;&gt; annual_avg_temp = pd.DataFrame(
     |  ...     {
     |  ...         "latitude": [1.2, 0.9, 1.9, 1.7, 5.0],
     |  ...         "longitude": [0.8, 1.1, 1.8, 1.8, 5.0],
     |  ...         "avg_temp": [10.0, 11.0, 15.0, 16.0, 20.0],
     |  ...     }
     |  ... )
     |  &gt;&gt;&gt; buildings
     |     latitude  longitude  n_stories
     |  0       1.0        1.0          3
     |  1       2.0        2.0          7
     |  &gt;&gt;&gt; annual_avg_temp
     |     latitude  longitude  avg_temp
     |  0       1.2        0.8      10.0
     |  1       0.9        1.1      11.0
     |  2       1.9        1.8      15.0
     |  3       1.7        1.8      16.0
     |  4       5.0        5.0      20.0
     |
     |  Let's interpolate the average temperature:
     |
     |  &gt;&gt;&gt; from sklearn.neighbors import KNeighborsRegressor
     |  &gt;&gt;&gt; from skrub import InterpolationJoiner
     |  &gt;&gt;&gt; InterpolationJoiner(
     |  ...     annual_avg_temp,
     |  ...     key=["latitude", "longitude"],
     |  ...     regressor=KNeighborsRegressor(2),
     |  ... ).fit_transform(buildings)
     |     latitude  longitude  n_stories  avg_temp
     |  0       1.0        1.0          3      10.5
     |  1       2.0        2.0          7      15.5
     |
     |  Method resolution order:
     |      InterpolationJoiner
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, aux_table, *, key=None, main_key=None, aux_key=None, suffix='', regressor=HistGradientBoostingRegressor(), classifier=HistGradientBoostingClassifier(), vectorizer=TableVectorizer(high_cardinality=MinHashEncoder()), n_jobs=None, on_estimator_failure='warn')
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Fit estimators to the ``aux_table`` provided during initialization.
     |
     |      ``X`` and ``y`` are mostly for scikit-learn compatibility.
     |
     |      Parameters
     |      ----------
     |      X : array-like or None
     |          The main table to which ``self.aux_table`` could be joined. If ``X``
     |          is not ``None``, an error is raised if any of the matching columns
     |          listed in ``self.main_key`` (or ``self.key``) are missing from ``X``.
     |
     |      y : array-like
     |          Ignored; only exists for compatibility with scikit-learn.
     |
     |      Returns
     |      -------
     |      InterpolationJoiner
     |          Fitted :class:`InterpolationJoiner` instance (self).
     |
     |  transform(self, X)
     |      Transform a table by joining inferred values to it.
     |
     |      The values of the ``main_key`` columns in ``X`` (the main table) are used
     |      to predict likely values for the contents of a matching row in
     |      ``aux_table`` (the auxiliary table).
     |
     |      Parameters
     |      ----------
     |      X : DataFrame
     |          The (main) table to transform.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The result of the join between ``X`` and inferred rows from ``aux_table``.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |
     |      Fits transformer to `X` and `y` with optional parameters `fit_params`
     |      and returns a transformed version of `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input samples.
     |
     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None
     |          Target values (None for unsupervised transformations).
     |
     |      **fit_params : dict
     |          Additional fit parameters.
     |
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class Joiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  Joiner(aux_table, *, key=None, main_key=None, aux_key=None, suffix='', max_dist=inf, ref_dist='random_pairs', string_encoder=Pipeline(steps=[('functiontransformer',
     |                   FunctionTransformer(func=functools.partial(&lt;function fill_nulls at 0x0000012A0D6F37E0&gt;, value=''))),
     |                  ('tostr', ToStr()),
     |                  ('hashingvectorizer',
     |                   HashingVectorizer(analyzer='char_wb', ngram_range=(2, 4))),
     |                  ('tfidftransformer', TfidfTransformer())]), add_match_info=True)
     |
     |  Augment features in a main table by fuzzy-joining an auxiliary table to it.
     |
     |  This transformer is initialized with an auxiliary table `aux_table`. It
     |  transforms a main table by joining it, with approximate ("fuzzy") matching,
     |  to the auxiliary table. The output of `transform` has the same rows as
     |  the main table (i.e. as the argument passed to `transform`), but each row
     |  is augmented with values from the best match in the auxiliary table.
     |
     |  To identify the best match for each row, values from the matching columns
     |  (`main_key` and `aux_key`) are vectorized, i.e. represented by vectors of
     |  continuous values. Then, the Euclidean distances between these vectors are
     |  computed to find, for each main table row, its nearest neighbor within the
     |  auxiliary table.
     |
     |  Optionally, a maximum distance threshold, `max_dist`, can be set. Matches
     |  between vectors that are separated by a distance (strictly) greater than
     |  `max_dist` will be rejected. We will consider that main table rows that
     |  are farther than `max_dist` from their nearest neighbor do not have a
     |  matching row in the auxiliary table, and the output will contain nulls for
     |  the entries that would normally have come from the auxiliary table (as in a
     |  traditional left join).
     |
     |  To make it easier to set a `max_dist` threshold, the distances are
     |  rescaled by dividing them by a reference distance, which can be chosen with
     |  `ref_dist`. The default is `'random_pairs'`. The possible choices are:
     |
     |  'random_pairs'
     |      Pairs of rows are sampled randomly from the auxiliary table and their
     |      distance is computed. The reference distance is the first quartile of
     |      those distances.
     |
     |  'second_neighbor'
     |      The reference distance is the distance to the *second* nearest neighbor
     |      in the auxiliary table.
     |
     |  'self_join_neighbor'
     |      Once the match candidate (i.e. the nearest neighbor from the auxiliary
     |      table) has been found, we find its nearest neighbor in the auxiliary
     |      table (excluding itself). The reference distance is the distance that
     |      separates those 2 auxiliary rows.
     |
     |  'no_rescaling'
     |      The reference distance is 1.0, i.e. no rescaling of the distances is
     |      applied.
     |
     |  Parameters
     |  ----------
     |  aux_table : dataframe
     |      The auxiliary table, which will be fuzzy-joined to the main table when
     |      calling `transform`.
     |  key : str or list of str, default=None
     |      The column names to use for both `main_key` and `aux_key` when they
     |      are the same. Provide either `key` or both `main_key` and `aux_key`.
     |  main_key : str or list of str, default=None
     |      The column names in the main table on which the join will be performed.
     |      Can be a string if joining on a single column.
     |      If `None`, `aux_key` must also be `None` and `key` must be provided.
     |  aux_key : str or list of str, default=None
     |      The column names in the auxiliary table on which the join will
     |      be performed. Can be a string if joining on a single column.
     |      If `None`, `main_key` must also be `None` and `key` must be provided.
     |  suffix : str, default=""
     |      Suffix to append to the `aux_table`'s column names. You can use it
     |      to avoid duplicate column names in the join.
     |  max_dist : int, float, `None` or `np.inf`, default=`np.inf`
     |      Maximum acceptable (rescaled) distance between a row in the
     |      `main_table` and its nearest neighbor in the `aux_table`. Rows that
     |      are farther apart are not considered to match. By default, the distance
     |      is rescaled so that a value between 0 and 1 is typically a good choice,
     |      although rescaled distances can be greater than 1 for some choices of
     |      `ref_dist`. `None`, `"inf"`, `float("inf")` or `numpy.inf`
     |      mean that no matches are rejected.
     |  ref_dist : reference distance for rescaling, default='random_pairs'
     |      Options are {"random_pairs", "second_neighbor", "self_join_neighbor",
     |      "no_rescaling"}. See above for a description of each option. To
     |      facilitate the choice of `max_dist`, distances between rows in
     |      `main_table` and their nearest neighbor in `aux_table` will be
     |      rescaled by this reference distance.
     |  string_encoder : scikit-learn transformer used to vectorize text columns
     |      By default a `HashingVectorizer` combined with a `TfidfTransformer`
     |      is used. Here we use raw TF-IDF features rather than transforming them
     |      for example with `GapEncoder` or `MinHashEncoder` because it is
     |      faster, these features are only used to find nearest neighbors and not
     |      used by downstream estimators, and distances between TF-IDF vectors
     |      have a somewhat simpler interpretation.
     |  add_match_info : bool, default=True
     |      Insert some columns whose names start with `skrub_Joiner` containing
     |      the distance, rescaled distance and whether the rescaled distance is
     |      above the threshold. Those values can be helpful for an estimator that
     |      uses the joined features, or to inspect the result of the join and set
     |      a `max_dist` threshold.
     |
     |  Attributes
     |  ----------
     |  max_dist_ : the maximum distance for a match to be accepted
     |      Equal to the parameter `max_dist` except that `"inf"` and `None`
     |      are mapped to `np.inf` (i.e. accept all matches).
     |
     |  vectorizer_ : scikit-learn ColumnTransformer
     |      The fitted transformer used to transform the matching columns into
     |      numerical vectors.
     |
     |  See Also
     |  --------
     |  AggJoiner :
     |      Aggregate an auxiliary dataframe before joining it on a base dataframe.
     |
     |  fuzzy_join :
     |      Join two tables (dataframes) based on approximate column matching. This
     |      is the same functionality as provided by the `Joiner` but exposed as
     |      a function rather than a transformer.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import Joiner
     |  &gt;&gt;&gt; main_table = pd.DataFrame({"Country": ["France", "Italia", "Georgia"]})
     |  &gt;&gt;&gt; aux_table = pd.DataFrame( {"Country": ["Germany", "France", "Italy"],
     |  ...                            "Capital": ["Berlin", "Paris", "Rome"]} )
     |  &gt;&gt;&gt; main_table
     |    Country
     |  0  France
     |  1  Italia
     |  2   Georgia
     |  &gt;&gt;&gt; aux_table
     |     Country Capital
     |  0  Germany  Berlin
     |  1   France   Paris
     |  2    Italy    Rome
     |  &gt;&gt;&gt; joiner = Joiner(
     |  ...     aux_table,
     |  ...     key="Country",
     |  ...     suffix="_aux",
     |  ...     max_dist=0.8,
     |  ...     add_match_info=False,
     |  ... )
     |  &gt;&gt;&gt; joiner.fit_transform(main_table)
     |    Country      Country_aux      Capital_aux
     |  0  France           France            Paris
     |  1  Italia            Italy             Rome
     |  2  Georgia              NaN              NaN
     |
     |  Method resolution order:
     |      Joiner
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, aux_table, *, key=None, main_key=None, aux_key=None, suffix='', max_dist=inf, ref_dist='random_pairs', string_encoder=Pipeline(steps=[('functiontransformer',
     |                   FunctionTransformer(func=functools.partial(&lt;function fill_nulls at 0x0000012A0D6F37E0&gt;, value=''))),
     |                  ('tostr', ToStr()),
     |                  ('hashingvectorizer',
     |                   HashingVectorizer(analyzer='char_wb', ngram_range=(2, 4))),
     |                  ('tfidftransformer', TfidfTransformer())]), add_match_info=True)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Fit the instance to the main table.
     |
     |      Parameters
     |      ----------
     |      X : DataFrame
     |          The main table, to be joined to the auxiliary ones.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      Joiner
     |          Fitted :class:`Joiner`instance (self).
     |
     |  fit_transform(self, X, y=None)
     |      Fit the instance to the main table.
     |
     |      Parameters
     |      ----------
     |      X : dataframe
     |          The main table, to be joined to the auxiliary ones.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The final joined table.
     |
     |  transform(self, X, y=None)
     |      Transform `X` using the specified encoding scheme.
     |
     |      Parameters
     |      ----------
     |      X : dataframe
     |          The main table, to be joined to the auxiliary ones.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      dataframe
     |          The final joined table.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  match_info_columns = ['skrub_Joiner_distance', 'skrub_Joiner_rescaled_...
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class MinHashEncoder(sklearn.base.TransformerMixin, skrub._on_each_column.SingleColumnTransformer)
     |  MinHashEncoder(*, n_components=30, ngram_range=(2, 4), hashing='fast', minmax_hash=False, n_jobs=None)
     |
     |  Encode string categorical features by applying the MinHash method to n-gram     decompositions of strings.
     |
     |  .. note::
     |
     |      ``MinHashEncoder`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((MinHashEncoder(), 'col_name_1'),
     |      (MinHashEncoder(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((MinHashEncoder(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  The principle is as follows:
     |
     |  1. A string is viewed as a succession of numbers (the ASCII or UTF8
     |     representation of its elements).
     |  2. The string is then decomposed into a set of n-grams, i.e.
     |     n-dimensional vectors of integers.
     |  3. A hashing function is used to assign an integer to each n-gram.
     |     The minimum of the hashes over all n-grams is used in the encoding.
     |  4. This process is repeated with `N` hashing functions to form
     |     N-dimensional encodings.
     |
     |  Maxhash encodings can be computed similarly by taking the maximum hash
     |  instead.
     |  With this procedure, strings that share many n-grams have a greater
     |  probability of having the same encoding value. These encodings thus capture
     |  morphological similarities between strings.
     |
     |  Parameters
     |  ----------
     |  n_components : int, default=30
     |      The number of dimension of encoded strings. Numbers around 300 tend to
     |      lead to good prediction performance, but with more computational cost.
     |  ngram_range : 2-tuple of int, default=(2, 4)
     |      The lower and upper boundaries of the range of n-values for different
     |      n-grams used in the string similarity. All values of `n` such
     |      that ``min_n &lt;= n &lt;= max_n`` will be used.
     |  hashing : {'fast', 'murmur'}, default='fast'
     |      Hashing function. `fast` is faster than `murmur` but
     |      might have some concern with its entropy.
     |  minmax_hash : bool, default=False
     |      If `True`, returns the min and max hashes concatenated.
     |  n_jobs : int, optional
     |      The number of jobs to run in parallel.
     |      The hash computations for all unique elements are parallelized.
     |      `None` means 1 unless in a joblib.parallel_backend.
     |      -1 means using all processors.
     |      See :term:`n_jobs` for more details.
     |
     |  Attributes
     |  ----------
     |  hash_dict_ : LRUDict
     |      Computed hashes.
     |  n_features_in_ : int
     |      Number of features seen during :term:`fit`.
     |  feature_names_in_ : ndarray of shape (n_features_in,)
     |      Names of features seen during :term:`fit`.
     |
     |  See Also
     |  --------
     |  GapEncoder
     |      Encodes dirty categories (strings) by constructing latent topics with
     |      continuous encoding.
     |  SimilarityEncoder
     |      Encode string columns as a numeric array with n-gram string similarity.
     |  StringEncoder
     |      Fast n-gram encoding of string columns.
     |  deduplicate
     |      Deduplicate data by hierarchically clustering similar strings.
     |
     |  References
     |  ----------
     |  For a detailed description of the method, see
     |  `Encoding high-cardinality string categorical variables
     |  &lt;https://hal.inria.fr/hal-02171256v4&gt;`_ by Cerda, Varoquaux (2019).
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import MinHashEncoder
     |  &gt;&gt;&gt; enc = MinHashEncoder(n_components=5)
     |
     |  Let's encode the following non-normalized data:
     |
     |  &gt;&gt;&gt; X = pd.Series(['paris, FR', 'Paris', 'London, UK', 'London'], name='city')
     |  &gt;&gt;&gt; enc.fit(X)
     |  MinHashEncoder(n_components=5)
     |
     |  The encoded data with 5 components are:
     |
     |  &gt;&gt;&gt; enc.transform(X)
     |           city_0        city_1        city_2        city_3        city_4
     |  0 -1.783375e+09 -1.588270e+09 -1.663592e+09 -1.819887e+09 -1.962594e+09
     |  1 -8.480470e+08 -1.766579e+09 -1.558912e+09 -1.485745e+09 -1.687299e+09
     |  2 -1.975829e+09 -2.095000e+09 -1.596521e+09 -1.817594e+09 -2.095693e+09
     |  3 -1.975829e+09 -2.095000e+09 -1.530721e+09 -1.459183e+09 -1.580988e+09
     |
     |  Method resolution order:
     |      MinHashEncoder
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, *, n_components=30, ngram_range=(2, 4), hashing='fast', minmax_hash=False, n_jobs=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Fit the MinHashEncoder to `X`.
     |
     |      In practice, just initializes a dictionary
     |      to store encodings to speed up computation.
     |
     |      Parameters
     |      ----------
     |      X : array-like, shape (n_samples, ) or (n_samples, n_columns)
     |          The string data to encode. Only here for compatibility.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      MinHashEncoder
     |          The fitted MinHashEncoder instance (self).
     |
     |  get_feature_names_out(self)
     |      Get output feature names for transformation.
     |
     |      Returns
     |      -------
     |      feature_names_out : ndarray of str objects
     |          Transformed feature names.
     |
     |  transform(self, X)
     |      Transform `X` using specified encoding scheme.
     |
     |      Parameters
     |      ----------
     |      X : array-like, shape (n_samples, ) or (n_samples, n_columns)
     |          The string data to encode.
     |
     |      Returns
     |      -------
     |      ndarray of shape (n_samples, n_columns * n_components)
     |          Transformed input.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |
     |      Fits transformer to `X` and `y` with optional parameters `fit_params`
     |      and returns a transformed version of `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input samples.
     |
     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None
     |          Target values (None for unsupervised transformations).
     |
     |      **fit_params : dict
     |          Additional fit parameters.
     |
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  set_fit_request(self: skrub._minhash_encoder.MinHashEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._minhash_encoder.MinHashEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class MultiAggJoiner(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  MultiAggJoiner(aux_tables, operations, *, keys=None, main_keys=None, aux_keys=None, cols=None, suffixes=None)
     |
     |  Extension of the :class:`AggJoiner` to multiple auxiliary tables.
     |
     |  Apply numerical and categorical aggregation operations on the `cols`
     |  to aggregate, selected by dtypes. See the list of supported operations
     |  at the parameter `operations`.
     |
     |  If `cols` is not provided, `cols` is set to a list of lists.
     |  For each table in `aux_tables`, the corresponding list will be all columns
     |  of that table, except the `aux_keys` associated with that table.
     |
     |  As opposed to the :class:`AggJoiner`, here `aux_tables` is an iterable of tables,
     |  each of which will be joined on the main table. Therefore `aux_keys` is now
     |  an iterable of keys, of the same length as `aux_tables`, and each entry
     |  in `aux_keys` is used to join the corresponding auxiliary table. In the same way,
     |  each entry in `cols` is an iterable of columns to aggregate in the corresponding
     |  auxiliary table. If the keys are the same in the main table and the auxiliary
     |  tables, the `keys` parameter can be used instead of `main_keys` and `aux_keys`.
     |
     |  Therefore if we have a single table, we could either use
     |
     |  - the :class:`AggJoiner`: ``AggJoiner(aux_table, key="ID")``
     |  - or the :class:`MultiAggJoiner`: ``MultiAggJoiner([aux_table], keys=[["ID"]])``
     |
     |  Note that for `keys`, `main_keys`, `aux_keys`, `cols` and `operations`,
     |  an input of the form ``[["a"], ["b"], ["c", "d"]]`` is valid
     |  while ``["a", "b", ["c", "d"]]`` is not.
     |
     |  Using a column from the first auxiliary table to join the second auxiliary table
     |  is not (yet) supported.
     |
     |  Accepts :obj:`pandas.DataFrame` and :class:`polars.DataFrame` inputs.
     |
     |  Parameters
     |  ----------
     |  aux_tables : iterable of DataFrameLike or "X"
     |      Auxiliary dataframes to aggregate then join on the base table.
     |      The placeholder string "X" can be provided to perform
     |      self-aggregation on the input data. To provide a single auxiliary table,
     |      ``aux_tables = [table]`` is supported, but not ``aux_tables = table``.
     |      It's possible to provide both the placeholder "X" and auxiliary tables,
     |      as in ``aux_tables = [table, "X"]``. If that's the case, the second table will
     |      be replaced by the input data.
     |
     |  operations : iterable of iterable of str
     |      Aggregation operations to perform on the auxiliary tables.
     |
     |      Must be an iterable of `operations` for each table in `aux_tables`.
     |      Supported operations are "count", "mode", "min", "max", "sum", "median",
     |      "mean", "std". The operations "sum", "median", "mean", "std" are reserved
     |      to numeric type columns.
     |
     |  keys : iterable of iterable of str, default=None
     |      The column names to use for both `main_keys` and `aux_key` when they
     |      are the same. Provide either `key` or both `main_keys` and `aux_keys`.
     |      If entries in `keys` contains multiple columns, we will perform
     |      a multi-column join.
     |
     |      All `keys` must be present in the main and auxiliary tables before fit.
     |      It's not (yet) possible to use columns from the first joined table
     |      to join the second.
     |
     |      If not `None`, there must be an iterable of `keys` for each table
     |      in `aux_tables`.
     |
     |  main_keys : iterable of iterable of str, default=None
     |      Select the columns from the main table to use as keys during
     |      the join operation.
     |      If entries in `main_keys` contains multiple columns, we will perform
     |      a multi-column join.
     |
     |      If not `None`, there must be an iterable of `main_keys` for each table
     |      in `aux_tables`.
     |
     |  aux_keys : iterable of iterable of str, default=None
     |      Select the columns from the auxiliary dataframes to use as keys during
     |      the join operation.
     |      If entries in `aux_keys` contains multiple columns, we will perform
     |      a multi-column join.
     |
     |      All `aux_keys` must be present in respective `aux_tables` before fit.
     |      It's not (yet) possible to use columns from the first joined table
     |      to join the second.
     |
     |      If not `None`, there must be an iterable of `aux_keys` for each table
     |      in `aux_tables`.
     |
     |  cols : iterable of iterable of str, default=None
     |      Select the columns from the auxiliary dataframes to use as values during
     |      the aggregation operations.
     |
     |      If not `None`, there must be an iterable of `cols` for each table
     |      in `aux_tables`.
     |
     |      If set to `None`, `cols` is set to a list of lists. For each table
     |      in `aux_tables`, the corresponding list will be all columns of that table,
     |      except the `aux_keys` associated with that table.
     |
     |  suffixes : iterable of str, default=None
     |      Suffixes to append to the `aux_tables`' column names.
     |      If set to `None`, the table indexes in `aux_tables` are used,
     |      e.g. for an aggregation of 2 `aux_tables`, "_0" and "_1" would be appended
     |      to column names.
     |
     |  See Also
     |  --------
     |  AggJoiner :
     |      Aggregate an auxiliary dataframe before joining it on a base dataframe.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import MultiAggJoiner
     |  &gt;&gt;&gt; patients = pd.DataFrame({
     |  ...    "patient_id": [1, 2],
     |  ...    "age": ["72", "45"],
     |  ... })
     |  &gt;&gt;&gt; hospitalizations = pd.DataFrame({
     |  ...    "visit_id": range(1, 7),
     |  ...    "patient_id": [1, 1, 1, 1, 2, 2],
     |  ...    "days_of_stay": [2, 4, 1, 1, 3, 12],
     |  ...    "hospital": ["Cochin", "Bichat", "Cochin", "Necker", "Bichat", "Bichat"],
     |  ... })
     |  &gt;&gt;&gt; medications = pd.DataFrame({
     |  ...    "medication_id": range(1, 6),
     |  ...    "patient_id": [1, 1, 1, 1, 2],
     |  ...    "medication": ["ozempic", "ozempic", "electrolytes", "ozempic", "morphine"],
     |  ... })
     |  &gt;&gt;&gt; glucose = pd.DataFrame({
     |  ...    "biology_id": range(1, 7),
     |  ...    "patientID": [1, 1, 1, 1, 2, 2],
     |  ...    "value": [1.4, 3.4, 1.0, 0.8, 3.1, 6.5],
     |  ... })
     |  &gt;&gt;&gt; multi_agg_joiner = MultiAggJoiner(
     |  ...    aux_tables=[hospitalizations, medications, glucose],
     |  ...    main_keys=[["patient_id"], ["patient_id"], ["patient_id"]],
     |  ...    aux_keys=[["patient_id"], ["patient_id"], ["patientID"]],
     |  ...    cols=[["days_of_stay"], ["medication"], ["value"]],
     |  ...    operations=[["max"], ["mode"], ["mean", "std"]],
     |  ...    suffixes=["", "", "_glucose"],
     |  ... )
     |  &gt;&gt;&gt; multi_agg_joiner.fit_transform(patients)
     |     patient_id  age  ...  value_mean_glucose  value_std_glucose
     |  0           1   72  ...                1.65           1.193035
     |  1           2   45  ...                4.80           2.404163
     |
     |  The :class:`MultiAggJoiner` makes it convenient to aggregate multiple tables, but
     |  the same results could be obtained by chaining 3 separate :class:`AggJoiner`:
     |
     |  &gt;&gt;&gt; from skrub import AggJoiner
     |  &gt;&gt;&gt; from sklearn.pipeline import make_pipeline
     |  &gt;&gt;&gt; agg_joiner_1 = AggJoiner(
     |  ...    aux_table=hospitalizations,
     |  ...    key="patient_id",
     |  ...    cols="days_of_stay",
     |  ...    operations="max",
     |  ... )
     |  &gt;&gt;&gt; agg_joiner_2 = AggJoiner(
     |  ...    aux_table=medications,
     |  ...    key="patient_id",
     |  ...    cols="medication",
     |  ...    operations="mode",
     |  ... )
     |  &gt;&gt;&gt; agg_joiner_3 = AggJoiner(
     |  ...    aux_table=glucose,
     |  ...    main_key="patient_id",
     |  ...    aux_key="patientID",
     |  ...    cols="value",
     |  ...    operations=["mean", "std"],
     |  ...    suffix="_glucose",
     |  ... )
     |  &gt;&gt;&gt; pipeline = make_pipeline(agg_joiner_1, agg_joiner_2, agg_joiner_3)
     |  &gt;&gt;&gt; pipeline.fit_transform(patients)
     |     patient_id  age  ...  value_mean_glucose  value_std_glucose
     |  0           1   72  ...                1.65           1.193035
     |  1           2   45  ...                4.80           2.404163
     |
     |  Method resolution order:
     |      MultiAggJoiner
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, aux_tables, operations, *, keys=None, main_keys=None, aux_keys=None, cols=None, suffixes=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Aggregate auxiliary tables based on the main keys.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          Input data, based table on which to left join the
     |          auxiliary tables.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      MultiAggJoiner
     |          Fitted :class:`MultiAggJoiner` instance (self).
     |
     |  fit_transform(self, X, y=None)
     |      Aggregate auxiliary tables based on the main keys.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          Input data, based table on which to left join the
     |          auxiliary tables.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The augmented input.
     |
     |  transform(self, X)
     |      Left-join pre-aggregated tables on `X`.
     |
     |      Parameters
     |      ----------
     |      X : DataFrameLike
     |          The input data to transform.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The augmented input.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class SelectCols(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  SelectCols(cols)
     |
     |  Select a subset of a DataFrame's columns.
     |
     |  A ``ValueError`` is raised if any of the provided column names are not in
     |  the dataframe.
     |
     |  Accepts :obj:`pandas.DataFrame` and :obj:`polars.DataFrame` inputs.
     |
     |  Parameters
     |  ----------
     |  cols : list of str or str
     |      The columns to select. A single column name can be passed as a ``str``:
     |      ``"col_name"`` is the same as ``["col_name"]``.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import SelectCols
     |  &gt;&gt;&gt; df = pd.DataFrame({"A": [1, 2], "B": [10, 20], "C": ["x", "y"]})
     |  &gt;&gt;&gt; df
     |     A   B  C
     |  0  1  10  x
     |  1  2  20  y
     |  &gt;&gt;&gt; SelectCols(["C", "A"]).fit_transform(df)
     |     C  A
     |  0  x  1
     |  1  y  2
     |  &gt;&gt;&gt; SelectCols(["X", "A"]).fit_transform(df)
     |  Traceback (most recent call last):
     |      ...
     |  ValueError: The following columns are requested for selection but missing from dataframe: ['X']
     |
     |  Method resolution order:
     |      SelectCols
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, cols)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit(self, X, y=None)
     |      Fit the transformer.
     |
     |      Parameters
     |      ----------
     |      X : DataFrame or None
     |          If `X` is a DataFrame, the transformer checks that all the column
     |          names provided in ``self.cols`` can be found in `X`.
     |
     |      y : None
     |          Unused.
     |
     |      Returns
     |      -------
     |      SelectCols
     |          The transformer itself.
     |
     |  transform(self, X)
     |      Transform a dataframe by selecting columns.
     |
     |      Parameters
     |      ----------
     |      X : DataFrame
     |          The DataFrame on which to apply the selection.
     |
     |      Returns
     |      -------
     |      DataFrame
     |          The input DataFrame ``X`` after selecting only the columns listed
     |          in ``self.cols`` (in the provided order).
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  __sklearn_tags__(self)
     |
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |
     |      Fits transformer to `X` and `y` with optional parameters `fit_params`
     |      and returns a transformed version of `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input samples.
     |
     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None
     |          Target values (None for unsupervised transformations).
     |
     |      **fit_params : dict
     |          Additional fit parameters.
     |
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class SimilarityEncoder(sklearn.preprocessing._encoders.OneHotEncoder)
     |  SimilarityEncoder(*, ngram_range=(2, 4), analyzer='char', categories='auto', dtype=&lt;class 'numpy.float64'&gt;, handle_unknown='ignore', handle_missing='', hashing_dim=None, n_jobs=None)
     |
     |  Encode string categories to a similarity matrix, to capture fuzziness across a few categories.
     |
     |  The input to this transformer should be an array-like of strings.
     |  The method is based on calculating the morphological similarities
     |  between the categories.
     |  This encoding is an alternative to OneHotEncoder for
     |  dirty categorical variables.
     |
     |  The principle of this encoder is as follows:
     |
     |  1. Given an input string array ``X = [x1, ..., xn]`` with `k` unique
     |     categories ``[c1, ..., ck]`` and a similarity measure ``sim(s1, s2)``
     |     between strings, we define the encoded vector of `xi` as
     |     ``[sim(xi, c1), ... , sim(xi, ck)]``.
     |     Similarity encoding of `X` results in a matrix with shape (`n`, `k`)
     |     that captures morphological similarities between string entries.
     |  2. To avoid dealing with high-dimensional encodings when `k` is high,
     |     we can use ``d &lt;&lt; k`` prototypes ``[p1, ..., pd]`` with which
     |     similarities will be computed:  ``xi -&gt; [sim(xi, p1), ..., sim(xi, pd)]``.
     |     These prototypes can be provided by the user. Otherwise, we recommend
     |     using the MinHashEncoder or GapEncoder when taking all unique entries
     |     leads to too many prototypes.
     |
     |  The similarity measure is based on the proportion of common n-grams between
     |  two strings.
     |
     |  Parameters
     |  ----------
     |  ngram_range : int 2-tuple (min_n, max_n), default=(2, 4)
     |      The lower and upper boundaries of the range of n-values for different
     |      n-grams used in the string similarity. All values of `n` such
     |      that ``min_n &lt;= n &lt;= max_n`` will be used.
     |  analyzer : {'word', 'char', 'char_wb'}, default='char'
     |      Analyzer parameter for the HashingVectorizer / CountVectorizer.
     |      Describes whether the matrix `V` to factorize should be made of
     |      word counts or character-level n-gram counts.
     |      Option ‘char_wb’ creates character n-grams only from text inside word
     |      boundaries; n-grams at the edges of words are padded with space.
     |  categories : {'auto'} or list of list of str
     |      Categories (unique values) per feature:
     |
     |      - 'auto' : Determine categories automatically from the training data.
     |      - list : `categories[i]` holds the categories expected in the i-th
     |        column. The passed categories must be sorted and should not mix
     |        strings and numeric values.
     |
     |      The categories used can be found in the SimilarityEncoder.categories_
     |      attribute.
     |  dtype : number type, default=float64
     |      Desired dtype of output.
     |  handle_unknown : 'error' or 'ignore', default=''
     |      Whether to raise an error or ignore if an unknown categorical feature
     |      is present during transform (default is to ignore). When this parameter
     |      is set to 'ignore' and an unknown category is encountered during
     |      transform, the resulting encoded columns for this feature
     |      will be all zeros. In the inverse transform, an unknown category
     |      will be denoted as None.
     |  handle_missing : 'error' or '', default=''
     |      Whether to raise an error or impute with blank string '' if missing
     |      values (NaN) are present during fit (default is to impute).
     |      When this parameter is set to '', and a missing value is encountered
     |      during fit_transform, the resulting encoded columns for this feature
     |      will be all zeros. In the inverse transform, the missing category
     |      will be denoted as None.
     |      "Missing values" are any value for which ``pandas.isna`` returns
     |      ``True``, such as ``numpy.nan`` or ``None``.
     |  hashing_dim : int, optional
     |      If `None`, the base vectorizer is a CountVectorizer, otherwise it is a
     |      HashingVectorizer with a number of features equal to `hashing_dim`.
     |  n_jobs : int, optional
     |      Maximum number of processes used to compute similarity matrices. Used
     |      only if `fast=True` in SimilarityEncoder.transform.
     |
     |  Attributes
     |  ----------
     |  categories_ : list of ndarray
     |      The categories of each feature determined during fitting
     |      (in the same order as the output of SimilarityEncoder.transform).
     |
     |  See Also
     |  --------
     |  MinHashEncoder :
     |      Encode string columns as a numeric array with the minhash method.
     |  GapEncoder :
     |      Encodes dirty categories (strings) by constructing latent topics
     |      with continuous encoding.
     |  TextEncoder :
     |      Encode string columns with a pretrained language model.
     |  deduplicate :
     |      Deduplicate data by hierarchically clustering similar strings.
     |
     |  Notes
     |  -----
     |  The functionality of SimilarityEncoder is easy to explain and understand,
     |  but it is not scalable. It is useful only to capture links across a few categories
     |  (eg eg: "west", "north", "north-west"), but not when there are many categories,
     |  as with open-ended entries.
     |  Instead, the GapEncoder is usually recommended.
     |
     |  References
     |  ----------
     |  For a detailed description of the method, see
     |  `Similarity encoding for learning with dirty categorical variables
     |  &lt;https://hal.inria.fr/hal-01806175&gt;`_ by Cerda, Varoquaux, Kegl. 2018
     |  (Machine Learning journal, Springer).
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; from skrub import SimilarityEncoder
     |  &gt;&gt;&gt; enc = SimilarityEncoder()
     |  &gt;&gt;&gt; X = [['Male', 1], ['Female', 3], ['Female', 2]]
     |  &gt;&gt;&gt; enc.fit(X)
     |  SimilarityEncoder()
     |
     |  It inherits the same methods as theOneHotEncoder:
     |
     |  &gt;&gt;&gt; enc.categories_
     |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]
     |
     |  But it provides a continuous encoding based on similarity
     |  instead of a discrete one based on exact matches:
     |
     |  &gt;&gt;&gt; enc.transform([['Female', 1], ['Male', 4]])
     |  array([[1.        , 0.42..., 1.        , 0.        , 0.        ],
     |         [0.42..., 1.        , 0.        , 0.        , 0.        ]])
     |  &gt;&gt;&gt; enc.get_feature_names_out(['gender', 'group'])
     |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],
     |        dtype=object)
     |
     |  Method resolution order:
     |      SimilarityEncoder
     |      sklearn.preprocessing._encoders.OneHotEncoder
     |      sklearn.preprocessing._encoders._BaseEncoder
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, *, ngram_range=(2, 4), analyzer='char', categories='auto', dtype=&lt;class 'numpy.float64'&gt;, handle_unknown='ignore', handle_missing='', hashing_dim=None, n_jobs=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __sklearn_tags__(self)
     |
     |  fit(self, X, y=None)
     |      Fit the instance to `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like, shape [n_samples, n_features]
     |          The data to determine the categories of each feature.
     |      y : None
     |          Unused, only here for compatibility.
     |
     |      Returns
     |      -------
     |      SimilarityEncoder
     |          The fitted SimilarityEncoder instance (self).
     |
     |  set_transform_request(self: skrub._similarity_encoder.SimilarityEncoder, *, fast: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._similarity_encoder.SimilarityEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``transform`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      fast : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``fast`` parameter in ``transform``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  transform(self, X, fast=True)
     |      Transform `X` using specified encoding scheme.
     |
     |      Parameters
     |      ----------
     |      X : array-like, shape [n_samples, n_features]
     |          The data to encode.
     |      fast : bool, default=True
     |          Whether to use the fast computation of ngrams.
     |
     |      Returns
     |      -------
     |      ndarray, shape [n_samples, n_features_new]
     |          Transformed input.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.preprocessing._encoders.OneHotEncoder:
     |
     |  get_feature_names_out(self, input_features=None)
     |      Get output feature names for transformation.
     |
     |      Parameters
     |      ----------
     |      input_features : array-like of str or None, default=None
     |          Input features.
     |
     |          - If `input_features` is `None`, then `feature_names_in_` is
     |            used as feature names in. If `feature_names_in_` is not defined,
     |            then the following input feature names are generated:
     |            `["x0", "x1", ..., "x(n_features_in_ - 1)"]`.
     |          - If `input_features` is an array-like, then `input_features` must
     |            match `feature_names_in_` if `feature_names_in_` is defined.
     |
     |      Returns
     |      -------
     |      feature_names_out : ndarray of str objects
     |          Transformed feature names.
     |
     |  inverse_transform(self, X)
     |      Convert the data back to the original representation.
     |
     |      When unknown categories are encountered (all zeros in the
     |      one-hot encoding), ``None`` is used to represent this category. If the
     |      feature with the unknown category has a dropped category, the dropped
     |      category will be its inverse.
     |
     |      For a given input feature, if there is an infrequent category,
     |      'infrequent_sklearn' will be used to represent the infrequent category.
     |
     |      Parameters
     |      ----------
     |      X : {array-like, sparse matrix} of shape                 (n_samples, n_encoded_features)
     |          The transformed data.
     |
     |      Returns
     |      -------
     |      X_tr : ndarray of shape (n_samples, n_features)
     |          Inverse transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Readonly properties inherited from sklearn.preprocessing._encoders._BaseEncoder:
     |
     |  infrequent_categories_
     |      Infrequent categories for each feature.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.TransformerMixin:
     |
     |  fit_transform(self, X, y=None, **fit_params)
     |      Fit to data, then transform it.
     |
     |      Fits transformer to `X` and `y` with optional parameters `fit_params`
     |      and returns a transformed version of `X`.
     |
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          Input samples.
     |
     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None
     |          Target values (None for unsupervised transformations).
     |
     |      **fit_params : dict
     |          Additional fit parameters.
     |
     |      Returns
     |      -------
     |      X_new : ndarray array of shape (n_samples, n_features_new)
     |          Transformed array.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class StringEncoder(skrub._on_each_column.SingleColumnTransformer)
     |  StringEncoder(n_components=30, vectorizer='tfidf', ngram_range=(3, 4), analyzer='char_wb')
     |
     |  Generate a lightweight string encoding of a given column using tf-idf         vectorization and truncated singular value decomposition (SVD).
     |
     |  .. note::
     |
     |      ``StringEncoder`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((StringEncoder(), 'col_name_1'),
     |      (StringEncoder(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((StringEncoder(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  First, apply a tf-idf vectorization of the text, then reduce the dimensionality
     |  with a truncated SVD with the given number of parameters.
     |
     |  New features will be named ``{col_name}_{component}`` if the series has a name,
     |  and ``tsvd_{component}`` if it does not.
     |
     |  Parameters
     |  ----------
     |  n_components : int, default=30
     |      Number of components to be used for the singular value decomposition (SVD).
     |      Must be a positive integer.
     |  vectorizer : str, "tfidf" or "hashing"
     |      Vectorizer to apply to the strings, either `tfidf` or `hashing` for
     |      scikit-learn TfidfVectorizer or HashingVectorizer respectively.
     |
     |  ngram_range : tuple of (int, int) pairs, default=(3,4)
     |      The lower and upper boundary of the range of n-values for different
     |      n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
     |      will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams,
     |      ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams.
     |
     |  analyzer : str, "char", "word" or "char_wb", default="char_wb"
     |      Whether the feature should be made of word or character n-grams.
     |      Option ``char_wb`` creates character n-grams only from text inside word
     |      boundaries; n-grams at the edges of words are padded with space.
     |
     |  See Also
     |  --------
     |  MinHashEncoder :
     |      Encode string columns as a numeric array with the minhash method.
     |  GapEncoder :
     |      Encode string columns by constructing latent topics.
     |  TextEncoder :
     |      Encode string columns using pre-trained language models.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import StringEncoder
     |
     |  We will encode the comments using 2 components:
     |
     |  &gt;&gt;&gt; enc = StringEncoder(n_components=2)
     |  &gt;&gt;&gt; X = pd.Series([
     |  ...   "The professor snatched a good interview out of the jaws of these questions.",
     |  ...   "Bookmarking this to watch later.",
     |  ...   "When you don't know the lyrics of the song except the chorus",
     |  ... ], name='video comments')
     |
     |  &gt;&gt;&gt; enc.fit_transform(X) # doctest: +SKIP
     |     video comments_0  video comments_1
     |  0      8.218069e-01      4.557474e-17
     |  1      6.971618e-16      1.000000e+00
     |  2      8.218069e-01     -3.046564e-16
     |
     |  Method resolution order:
     |      StringEncoder
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, n_components=30, vectorizer='tfidf', ngram_range=(3, 4), analyzer='char_wb')
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit_transform(self, X, y=None)
     |      Fit the encoder and transform a column.
     |
     |      Parameters
     |      ----------
     |      X : Pandas or Polars series
     |          The column to transform.
     |      y : None
     |          Unused. Here for compatibility with scikit-learn.
     |
     |      Returns
     |      -------
     |      X_out: Pandas or Polars dataframe with shape (len(X), tsvd_n_components)
     |          The embedding representation of the input.
     |
     |  get_feature_names_out(self)
     |      Get output feature names for transformation.
     |
     |      Returns
     |      -------
     |      feature_names_out : list of str objects
     |          Transformed feature names.
     |
     |  set_fit_request(self: skrub._string_encoder.StringEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._string_encoder.StringEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  transform(self, X)
     |      Transform a column.
     |
     |      Parameters
     |      ----------
     |      X : Pandas or Polars series
     |          The column to transform.
     |
     |      Returns
     |      -------
     |      result: Pandas or Polars dataframe with shape (len(X), tsvd_n_components)
     |          The embedding representation of the input.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  fit(self, column, y=None)
     |      Fit the transformer.
     |
     |      Subclasses should implement ``fit_transform`` and ``transform``.
     |
     |      Parameters
     |      ----------
     |      column : a pandas or polars Series
     |          Unlike most scikit-learn transformers, single-column transformers
     |          transform a single column, not a whole dataframe.
     |
     |      y : column or dataframe
     |          Prediction targets.
     |
     |      Returns
     |      -------
     |      self
     |          The fitted transformer.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __init_subclass__(**kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  __sklearn_tags__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class TableReport(builtins.object)
     |  TableReport(dataframe, n_rows=10, order_by=None, title=None, column_filters=None, verbose=1)
     |
     |  Summarize the contents of a dataframe.
     |
     |  This class summarizes a dataframe, providing information such as the type
     |  and summary statistics (mean, number of missing values, etc.) for each
     |  column.
     |
     |  Parameters
     |  ----------
     |  dataframe : pandas or polars DataFrame
     |      The dataframe to summarize.
     |  n_rows : int, default=10
     |      Maximum number of rows to show in the sample table. Half will be taken
     |      from the beginning (head) of the dataframe and half from the end
     |      (tail). Note this is only for display. Summary statistics, histograms
     |      etc. are computed using the whole dataframe.
     |  order_by : str
     |      Column name to use for sorting. Other numerical columns will be plotted
     |      as function of the sorting column. Must be of numerical or datetime
     |      type.
     |  title : str
     |      Title for the report.
     |  column_filters : dict
     |      A dict for adding custom entries to the column filter dropdown menu.
     |      Each key is an id for the filter (e.g. ``"first_10"``) and the value is a
     |      mapping with the keys ``display_name`` (the name shown in the menu,
     |      e.g. ``"First 10 columns"``) and ``columns`` (a list of column names).
     |      See the end of the "Examples" section below for details.
     |  verbose : int, default = 1
     |      Whether to print progress information while the report is being generated.
     |
     |      * verbose = 1 prints how many columns have been processed so far.
     |      * verbose = 0 silences the output.
     |
     |  See Also
     |  --------
     |  patch_display :
     |      Replace the default DataFrame HTML displays in the output of notebook
     |      cells with a TableReport.
     |
     |  Notes
     |  -----
     |  You can see some `example reports`_ for a few datasets online. We also
     |  provide an experimental online demo_ that allows you to select a CSV or
     |  parquet file and generate a report directly in your web browser.
     |
     |  .. _example reports: https://skrub-data.org/skrub-reports/examples/
     |  .. _demo: https://skrub-data.org/skrub-reports/
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import TableReport
     |  &gt;&gt;&gt; df = pd.DataFrame(dict(a=[1, 2], b=['one', 'two'], c=[11.1, 11.1]))
     |  &gt;&gt;&gt; report = TableReport(df)
     |
     |  If you are in a Jupyter notebook, to display the report just have it be the
     |  last expression evaluated in a cell so that it is displayed in the cell's
     |  output.
     |
     |  &gt;&gt;&gt; report
     |  &lt;TableReport: use .open() to display&gt;
     |
     |  (Note that above we only see the string representation, not the report itself,
     |  because we are not in a notebook.)
     |
     |  Whether you are using a notebook or not, you can always open the report as a
     |  full page in a separate browser tab with its ``open`` method:
     |  ``report.open()``.
     |
     |  You can also get the HTML report as a string.
     |  For a full, standalone web page:
     |
     |  &gt;&gt;&gt; report.html()
     |  Processing...
     |  '&lt;!DOCTYPE html&gt;\n&lt;html lang="en-US"&gt;\n\n&lt;head&gt;\n    &lt;meta charset="utf-8"...'
     |
     |  For an HTML fragment that can be inserted into a page:
     |
     |  &gt;&gt;&gt; report.html_snippet()
     |  '\n&lt;div id="report_...-wrapper" hidden&gt;\n    &lt;template id="report_...'
     |
     |  Advanced configuration: you can add custom column filters that will appear
     |  in the report's dropdown menu.
     |
     |  &gt;&gt;&gt; filters = {
     |  ...     "at_least_2": {
     |  ...         "display_name": "Columns with at least 2 unique values",
     |  ...         "columns": ["a", "b"],
     |  ...     }
     |  ... }
     |  &gt;&gt;&gt; report = TableReport(df, column_filters=filters)
     |
     |  With the code above, in addition to the default filters such as "All
     |  columns", "Numeric columns", etc., the added "Columns with at least 2
     |  unique values" will be available in the report, selecting columns "a" and
     |  "b".
     |
     |  Methods defined here:
     |
     |  __init__(self, dataframe, n_rows=10, order_by=None, title=None, column_filters=None, verbose=1)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __repr__(self)
     |      Return repr(self).
     |
     |  html(self)
     |      Get the report as a full HTML page.
     |
     |      Returns
     |      -------
     |      str :
     |          The HTML page.
     |
     |  html_snippet(self)
     |      Get the report as an HTML fragment that can be inserted in a page.
     |
     |      Returns
     |      -------
     |      str :
     |          The HTML snippet.
     |
     |  json(self)
     |      Get the report data in JSON format.
     |
     |      Returns
     |      -------
     |      str :
     |          The JSON data.
     |
     |  open(self)
     |      Open the HTML report in a web browser.
     |
     |  write_html(self, file)
     |      Store the report into an HTML file.
     |
     |      Parameters
     |      ----------
     |      file : str, pathlib.Path or file object
     |          The file object or path of the file to store the HTML output.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object

    class TableVectorizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)
     |  TableVectorizer(*, cardinality_threshold=40, low_cardinality=OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
     |                sparse_output=False), high_cardinality=GapEncoder(n_components=30), numeric=PassThrough(), datetime=DatetimeEncoder(), specific_transformers=(), drop_null_fraction=1.0, n_jobs=None)
     |
     |  Transform a dataframe to a numeric (vectorized) representation.
     |
     |  Applies a different transformation to each of several kinds of columns:
     |
     |  - `numeric`: floats, integers, and booleans.
     |  - `datetime`: datetimes and dates.
     |  - `low_cardinality`: string and categorical columns with a count
     |    of unique values smaller than a given threshold (40 by default). Category encoding
     |    schemes such as one-hot encoding, ordinal encoding etc. are typically appropriate
     |    for columns with few unique values.
     |  - `high_cardinality`: string and categorical columns with many
     |    unique values, such as free-form text. Such columns have so many distinct values
     |    that it is not possible to assign a distinct representation to each: the dimension
     |    would be too large and there would be too few examples of each category.
     |    Representations designed for text, such as topic modelling
     |    (:class:`~skrub.GapEncoder`) or locality-sensitive hashing
     |    (:class:`~skrub.MinHash`) are more appropriate.
     |
     |  .. note::
     |
     |      Transformations are applied **independently on each column**. A
     |      different transformer instance is used for each column separately;
     |      multivariate transformations are therefore not supported.
     |
     |  The transformer for each kind of column can be configured with the corresponding
     |  parameter. A transformer is expected to be a `compatible scikit-learn transformer
     |  &lt;https://scikit-learn.org/stable/glossary.html#term-transformer&gt;`_. Special-cased
     |  strings ``"drop"`` and ``"passthrough"`` are accepted as well, to indicate to drop
     |  the columns or to pass them through untransformed, respectively.
     |
     |  Additionally, it is possible to specify transformers for specific columns,
     |  overriding the categorization described above. This is done by providing a
     |  list of pairs ``(transformer, list_of_columns)`` as the
     |  ``specific_transformers`` parameter.
     |
     |  .. note::
     |
     |      The ``specific_transformers`` parameter will be removed in a future
     |      version of ``skrub``, when better utilities for building complex
     |      pipelines are introduced.
     |
     |  Parameters
     |  ----------
     |  cardinality_threshold : int, default=40
     |      String and categorical columns with a number of unique values strictly smaller
     |      than this threshold are handled by the transformer ``low_cardinality``, the rest
     |      are handled by the transformer ``high_cardinality``.
     |
     |  low_cardinality : transformer, "passthrough" or "drop",             default=OneHotEncoder instance
     |      The transformer for string or categorical columns with strictly fewer than
     |      ``cardinality_threshold`` unique values. By default, we use a
     |      :class:`~sklearn.preprocessing.OneHotEncoder` that ignores unknown categories
     |      and drop one of the transformed columns if the feature contains only 2
     |      categories.
     |
     |  high_cardinality : transformer, "passthrough" or "drop", default=GapEncoder instance
     |      The transformer for string or categorical columns with at least
     |      ``cardinality_threshold`` unique values. The default is a
     |      :class:`~skrub.GapEncoder` with 30 components (30 output columns for each
     |      input).
     |
     |  numeric : transformer, "passthrough" or "drop", default="passthrough"
     |      The transformer for numeric columns (floats, ints, booleans).
     |
     |  datetime : transformer, "passthrough" or "drop", default=DatetimeEncoder instance
     |      The transformer for date and datetime columns. By default, we use a
     |      :class:`~skrub.DatetimeEncoder`.
     |
     |  specific_transformers : list of (transformer, list of column names) pairs,             default=()
     |      Override the categories above for the given columns and force using the
     |      specified transformer. This disables any preprocessing usually done by
     |      the TableVectorizer; the columns are passed to the transformer without
     |      any modification. A column is not allowed to appear twice in
     |      ``specific_transformers``. Using ``specific_transformers`` provides
     |      similar functionality to what is offered by scikit-learn's
     |      :class:`~sklearn.compose.ColumnTransformer`.
     |
     |  drop_null_fraction : float or None, default=1.0
     |      Fraction of null above which the column is dropped. If `drop_null_fraction` is
     |      set to ``1.0``, the column is dropped if it contains only
     |      nulls or NaNs (this is the default behavior). If `drop_null_fraction` is a
     |      number in ``[0.0, 1.0)``, the column is dropped if the fraction of nulls
     |      is strictly larger than `drop_null_fraction`. If `drop_null_fraction` is ``None``,
     |      this selection is disabled: no columns are dropped based on the number
     |      of null values they contain.
     |
     |  n_jobs : int, default=None
     |      Number of jobs to run in parallel.
     |      ``None`` means 1 unless in a joblib ``parallel_backend`` context.
     |      ``-1`` means using all processors.
     |
     |  Attributes
     |  ----------
     |  transformers_ : dict
     |      Maps the name of each column to the fitted transformer that was applied
     |      to it.
     |
     |  column_to_kind_ : dict
     |      Maps each column name to the kind (``"high_cardinality"``,
     |      ``"low_cardinality"``, ``"specific"``, etc.) it was assigned.
     |
     |  kind_to_columns_ : dict
     |      The reverse of ``column_to_kind_``: maps each kind of column
     |      (``"high_cardinality"``, ``"low_cardinality"``, etc.) to a list of
     |      column names. For example ``kind_to_columns['datetime']`` contains the
     |      names of all datetime columns.
     |
     |  input_to_outputs_ : dict
     |      Maps the name of each input column to the names of the corresponding
     |      output columns.
     |
     |  output_to_input_ : dict
     |      The reverse of ``input_to_outputs_``: maps the name of each output
     |      column to the name of the column in the input dataframe from which it
     |      was derived.
     |
     |  all_processing_steps_ : dict
     |      Maps the name of each column to a list of all the processing steps that were
     |      applied to it. Those steps may include some pre-processing transformations such
     |      as converting strings to datetimes or numbers, the main transformer (e.g. the
     |      :class:`~skrub.DatetimeEncoder`), and a post-processing step casting the main
     |      transformer's output to :obj:`numpy.float32`. See the "Examples" section below
     |      for details.
     |
     |  feature_names_in_ : list of str
     |      The names of the input columns, after applying some cleaning (casting
     |      all column names to strings and deduplication).
     |
     |  n_features_in_ : int
     |      The number of input columns.
     |
     |  all_outputs_ : list of str
     |      The names of the output columns.
     |
     |  See Also
     |  --------
     |  tabular_learner :
     |      A function that accepts a scikit-learn estimator and creates a pipeline
     |      combining a ``TableVectorizer``, optional missing value imputation and
     |      the provided estimator.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; from skrub import TableVectorizer
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; df = pd.DataFrame({
     |  ...     'A': ['one', 'two', 'two', 'three'],
     |  ...     'B': ['02/02/2024', '23/02/2024', '12/03/2024', '13/03/2024'],
     |  ...     'C': ['1.5', 'N/A', '12.2', 'N/A'],
     |  ... })
     |  &gt;&gt;&gt; df
     |         A           B     C
     |  0    one  02/02/2024   1.5
     |  1    two  23/02/2024   N/A
     |  2    two  12/03/2024  12.2
     |  3  three  13/03/2024   N/A
     |  &gt;&gt;&gt; df.dtypes
     |  A    object
     |  B    object
     |  C    object
     |  dtype: object
     |
     |  &gt;&gt;&gt; vectorizer = TableVectorizer()
     |  &gt;&gt;&gt; vectorizer.fit_transform(df)
     |     A_one  A_three  A_two  B_year  B_month  B_day  B_total_seconds     C
     |  0    1.0      0.0    0.0  2024.0      2.0    2.0     1.706832e+09   1.5
     |  1    0.0      0.0    1.0  2024.0      2.0   23.0     1.708646e+09   NaN
     |  2    0.0      0.0    1.0  2024.0      3.0   12.0     1.710202e+09  12.2
     |  3    0.0      1.0    0.0  2024.0      3.0   13.0     1.710288e+09   NaN
     |
     |  We can inspect which outputs were created from a given column in the input
     |  dataframe:
     |
     |  &gt;&gt;&gt; vectorizer.input_to_outputs_['B']
     |  ['B_year', 'B_month', 'B_day', 'B_total_seconds']
     |
     |  and the reverse mapping:
     |
     |  &gt;&gt;&gt; vectorizer.output_to_input_['B_total_seconds']
     |  'B'
     |
     |  We can also see the encoder that was applied to a given column:
     |
     |  &gt;&gt;&gt; vectorizer.transformers_['B']
     |  DatetimeEncoder()
     |  &gt;&gt;&gt; vectorizer.transformers_['A']
     |  OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
     |                sparse_output=False)
     |  &gt;&gt;&gt; vectorizer.transformers_['A'].categories_
     |  [array(['one', 'three', 'two'], dtype=object)]
     |
     |  We can see the columns grouped by the kind of encoder that was applied
     |  to them:
     |
     |  &gt;&gt;&gt; vectorizer.kind_to_columns_
     |  {'numeric': ['C'], 'datetime': ['B'], 'low_cardinality': ['A'], 'high_cardinality': [], 'specific': []}
     |
     |  As well as the reverse mapping (from each column to its kind):
     |
     |  &gt;&gt;&gt; vectorizer.column_to_kind_
     |  {'C': 'numeric', 'B': 'datetime', 'A': 'low_cardinality'}
     |
     |  Before applying the main transformer, the ``TableVectorizer`` applies
     |  several preprocessing steps, for example to detect numbers or dates that are
     |  represented as strings. By default, columns that contain only null values are
     |  dropped. Moreover, a final post-processing step is applied to all
     |  non-categorical columns in the encoder's output to cast them to float32.
     |  We can inspect all the processing steps that were applied to a given column:
     |
     |  &gt;&gt;&gt; vectorizer.all_processing_steps_['B']
     |  [CleanNullStrings(), DropIfTooManyNulls(), ToDatetime(), DatetimeEncoder(), {'B_day': ToFloat32(), 'B_month': ToFloat32(), ...}]
     |
     |  Note that as the encoder (``DatetimeEncoder()`` above) produces multiple
     |  columns, the last processing step is not described by a single transformer
     |  like the previous ones but by a mapping from column name to transformer.
     |
     |  ``all_processing_steps_`` is useful to inspect the details of the
     |  choices made by the ``TableVectorizer`` during preprocessing, for example:
     |
     |  &gt;&gt;&gt; vectorizer.all_processing_steps_['B'][2]
     |  ToDatetime()
     |  &gt;&gt;&gt; _.format_
     |  '%d/%m/%Y'
     |
     |  **Transformers are applied separately to each column**
     |
     |  The ``TableVectorizer`` vectorizes each column separately -- a different
     |  transformer is applied to each column; multivariate transformers are not
     |  allowed.
     |
     |  &gt;&gt;&gt; df_1 = pd.DataFrame(dict(A=['one', 'two'], B=['three', 'four']))
     |  &gt;&gt;&gt; vectorizer = TableVectorizer().fit(df_1)
     |  &gt;&gt;&gt; vectorizer.transformers_['A'] is not vectorizer.transformers_['B']
     |  True
     |  &gt;&gt;&gt; vectorizer.transformers_['A'].categories_
     |  [array(['one', 'two'], dtype=object)]
     |  &gt;&gt;&gt; vectorizer.transformers_['B'].categories_
     |  [array(['four', 'three'], dtype=object)]
     |
     |  **Overriding the transformer for specific columns**
     |
     |  We can also provide transformers for specific columns. In that case the
     |  provided transformer has full control over the associated columns; no other
     |  processing is applied to those columns. A column cannot appear twice in the
     |  ``specific_transformers``.
     |
     |  .. note::
     |
     |      This functionality is likely to be removed in a future version of the
     |      ``TableVectorizer``.
     |
     |  The overrides are provided as a list of pairs:
     |  ``(transformer, list_of_column_names)``.
     |
     |  &gt;&gt;&gt; from sklearn.preprocessing import OrdinalEncoder
     |  &gt;&gt;&gt; vectorizer = TableVectorizer(
     |  ...     specific_transformers=[('drop', ['A']), (OrdinalEncoder(), ['B'])]
     |  ... )
     |  &gt;&gt;&gt; df
     |         A           B     C
     |  0    one  02/02/2024   1.5
     |  1    two  23/02/2024   N/A
     |  2    two  12/03/2024  12.2
     |  3  three  13/03/2024   N/A
     |  &gt;&gt;&gt; vectorizer.fit_transform(df)
     |       B     C
     |  0  0.0   1.5
     |  1  3.0   NaN
     |  2  1.0  12.2
     |  3  2.0   NaN
     |
     |  Here the column 'A' has been dropped and the column 'B' has been passed to
     |  the ``OrdinalEncoder`` (instead of the default choice which would have been
     |  ``DatetimeEncoder``).
     |
     |  We can see that 'A' and 'B' are now treated as 'specific' columns:
     |
     |  &gt;&gt;&gt; vectorizer.column_to_kind_
     |  {'C': 'numeric', 'A': 'specific', 'B': 'specific'}
     |
     |  Preprocessing and postprocessing steps are not applied to columns appearing
     |  in ``specific_columns``. For example 'B' has not gone through
     |  ``ToDatetime()``:
     |
     |  &gt;&gt;&gt; vectorizer.all_processing_steps_
     |  {'A': [Drop()], 'B': [OrdinalEncoder()], 'C': [CleanNullStrings(), DropIfTooManyNulls(), ToFloat32(), PassThrough(), {'C': ToFloat32()}]}
     |
     |  Specifying several ``specific_transformers`` for the same column is not allowed.
     |
     |  &gt;&gt;&gt; vectorizer = TableVectorizer(
     |  ...     specific_transformers=[('passthrough', ['A', 'B']), ('drop', ['A'])]
     |  ... )
     |
     |  &gt;&gt;&gt; vectorizer.fit_transform(df)
     |  Traceback (most recent call last):
     |      ...
     |  ValueError: Column 'A' used twice in 'specific_transformers', at indices 0 and 1.
     |
     |  Method resolution order:
     |      TableVectorizer
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, *, cardinality_threshold=40, low_cardinality=OneHotEncoder(drop='if_binary', dtype='float32', handle_unknown='ignore',
     |                sparse_output=False), high_cardinality=GapEncoder(n_components=30), numeric=PassThrough(), datetime=DatetimeEncoder(), specific_transformers=(), drop_null_fraction=1.0, n_jobs=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  __sklearn_tags__(self)
     |
     |  fit(self, X, y=None)
     |      Fit transformer.
     |
     |      Parameters
     |      ----------
     |      X : dataframe of shape (n_samples, n_features)
     |          Input data to transform.
     |
     |      y : array-like, shape (n_samples,) or (n_samples, n_outputs) or None,                 default=None
     |          Target values for supervised learning (None for unsupervised
     |          transformations).
     |
     |      Returns
     |      -------
     |      self : TableVectorizer
     |          The fitted estimator.
     |
     |  fit_transform(self, X, y=None)
     |      Fit transformer and transform dataframe.
     |
     |      Parameters
     |      ----------
     |      X : dataframe of shape (n_samples, n_features)
     |          Input data to transform.
     |
     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None,                 default=None
     |          Target values for supervised learning (None for unsupervised
     |          transformations).
     |
     |      Returns
     |      -------
     |      dataframe
     |          The transformed input.
     |
     |  get_feature_names_out(self)
     |      Return the column names of the output of ``transform`` as a list of strings.
     |
     |      Returns
     |      -------
     |      list of strings
     |          The column names.
     |
     |  transform(self, X)
     |      Transform dataframe.
     |
     |      Parameters
     |      ----------
     |      X : dataframe of shape (n_samples, n_features)
     |          Input data to transform.
     |
     |      Returns
     |      -------
     |      dataframe
     |          The transformed input.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __init_subclass__(auto_wrap_output_keys=('transform',), **kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class TextEncoder(skrub._on_each_column.SingleColumnTransformer, sklearn.base.TransformerMixin)
     |  TextEncoder(model_name='intfloat/e5-small-v2', n_components=30, device=None, batch_size=32, token_env_variable=None, cache_folder=None, store_weights_in_pickle=False, random_state=None, verbose=False)
     |
     |  Encode string features by applying a pretrained language model         downloaded from the HuggingFace Hub.
     |
     |  .. note::
     |
     |      ``TextEncoder`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((TextEncoder(), 'col_name_1'),
     |      (TextEncoder(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((TextEncoder(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  This is a thin wrapper around :class:`~sentence_transformers.SentenceTransformer`
     |  that follows the scikit-learn API, making it usable within a scikit-learn pipeline.
     |
     |  .. warning::
     |
     |      To use this class, you need to install the optional ``transformers``
     |      dependencies for skrub. See the "deep learning dependencies" section
     |      in the :ref:`installation_instructions` guide for more details.
     |
     |  This class uses a pre-trained model, so calling ``fit`` or ``fit_transform``
     |  will not train or fine-tune the model. Instead, the model is loaded from disk,
     |  and a PCA is fitted to reduce the dimension of the language model's output,
     |  if ``n_components`` is not None.
     |
     |  When PCA is disabled, this class is essentially stateless, with loading the
     |  pre-trained model from disk being the only difference between ``fit_transform``
     |  and ``transform``.
     |
     |  Be aware that parallelizing this class (e.g., using
     |  :class:`~skrub.TableVectorizer` with ``n_jobs`` &gt; 1) may be computationally
     |  expensive. This is because a copy of the pre-trained model is loaded into memory
     |  for each thread. Therefore, we recommend you to let the default n_jobs=None
     |  (or set to 1) of the TableVectorizer and let pytorch handle parallelism.
     |
     |  If memory usage is a concern, check the characteristics of your selected model.
     |
     |  Parameters
     |  ----------
     |  model_name : str, default="intfloat/e5-small-v2"
     |
     |      - If a filepath on disk is passed, this class loads the model from that path.
     |      - Otherwise, it first tries to download a pre-trained
     |        :class:`~sentence_transformers.SentenceTransformer` model.
     |        If that fails, tries to construct a model from Huggingface models repository
     |        with that name.
     |
     |      The following models have a good performance/memory usage tradeoff:
     |
     |      - ``intfloat/e5-small-v2``
     |      - ``all-MiniLM-L6-v2``
     |      - ``all-mpnet-base-v2``
     |
     |      You can find more options on the `sentence-transformers documentation
     |      &lt;https://www.sbert.net/docs/pretrained_models.html#model-overview&gt;`_.
     |
     |      The default model is a shrunk version of e5-v2, which has shown good
     |      performance in the benchmark of [1]_.
     |
     |  n_components : int or None, default=30,
     |      The number of embedding dimensions. As the number of dimensions is different
     |      across embedding models, this class uses a :class:`~sklearn.decomposition.PCA`
     |      to set the number of embedding to ``n_components`` during ``transform``.
     |      Set ``n_components=None`` to skip the PCA dimension reduction mechanism.
     |
     |      See [1]_ for more details on the choice of the PCA and default
     |      ``n_components``.
     |
     |  device : str, default=None
     |      Device (e.g. "cpu", "cuda", "mps") that should be used for computation.
     |      If None, checks if a GPU can be used.
     |      Note that macOS ARM64 users can enable the GPU on their local machine
     |      by setting ``device="mps"``.
     |
     |  batch_size : int, default=32
     |      The batch size to use during ``transform``.
     |
     |  token_env_variable : str, default=None
     |      The name of the environment variable which stores your HuggingFace
     |      authentication token to download private models.
     |      Note that we only store the name of the variable but not the token itself.
     |
     |  cache_folder : str, default=None
     |      Path to store models. By default ``~/skrub_data``.
     |      See :func:`skrub.datasets._utils.get_data_dir`.
     |      Note that when unpickling ``TextEncoder`` on another machine,
     |      the ``cache_folder`` path needs to be accessible to store the downloaded model.
     |
     |  store_weights_in_pickle : bool, default=False
     |      Whether or not to keep the loaded sentence-transformers model
     |      in the ``TextEncoder`` when pickling.
     |
     |      - When set to False, the ``_estimator`` property is removed from
     |        the object to pickle, which significantly reduces the size of
     |        the serialized object. Note that when the serialized object is
     |        unpickled on another machine, the ``TextEncoder`` will try to download
     |        the sentence-transformer model again from HuggingFace Hub.
     |        This process could fail if, for example, the machine doesn't have
     |        internet access. Additionally, if you use weights stored on disk
     |        that are *not* on the HuggingFace Hub (by passing a path to
     |        ``model_name``), these weights will not be pickled either.
     |        Therefore you would need to copy them to the machine where you
     |        unpickle the ``TextEncoder``.
     |      - When set to True, the ``_estimator`` property is included in
     |        the serialized object. Users deploying fine-tuned models stored on
     |        disk are recommended to use this option. Note that the machine
     |        where the ``TextEncoder`` is unpickled must have the same device than
     |        the machine where it was pickled.
     |
     |  random_state : int, RandomState instance or None, default=None
     |      Used when the PCA dimension reduction mechanism is used, for reproducible
     |      results across multiple function calls.
     |
     |  verbose : bool, default=True
     |      Verbose level, controls whether to show a progress bar or not during
     |      ``transform``.
     |
     |  Attributes
     |  ----------
     |  input_name_ : str
     |      The name of the fitted column.
     |
     |  pca_ : sklearn.decomposition.PCA
     |      A fitted PCA to reduce the embedding dimensionality (either PCA or truncation,
     |      see the ``n_components`` parameter).
     |
     |  n_components_ : int
     |      The number of dimensions of the embeddings after dimensionality
     |      reduction.
     |
     |  See Also
     |  --------
     |  MinHashEncoder :
     |      Encode string columns as a numeric array with the minhash method.
     |  GapEncoder :
     |      Encode string columns by constructing latent topics.
     |  StringEncoder
     |      Fast n-gram encoding of string columns.
     |  SimilarityEncoder :
     |      Encode string columns as a numeric array with n-gram string similarity.
     |
     |  References
     |  ----------
     |  .. [1]  L. Grinsztajn, M. Kim, E. Oyallon, G. Varoquaux
     |          "Vectorizing string entries for data processing on tables: when are larger
     |          language models better?", 2023.
     |          https://hal.science/hal-04345931
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import TextEncoder
     |
     |  Let's encode video comments using only 2 embedding dimensions:
     |
     |  &gt;&gt;&gt; enc = TextEncoder(model_name='intfloat/e5-small-v2', n_components=2)
     |  &gt;&gt;&gt; X = pd.Series([
     |  ...   "The professor snatched a good interview out of the jaws of these questions.",
     |  ...   "Bookmarking this to watch later.",
     |  ...   "When you don't know the lyrics of the song except the chorus",
     |  ... ], name='video comments')
     |
     |  Fitting does not train the underlying pre-trained deep-learning model,
     |  but ensure various checks and enable dimension reduction.
     |
     |  &gt;&gt;&gt; enc.fit_transform(X) # doctest: +SKIP
     |     video comments_1  video comments_2
     |  0          0.411395          0.096504
     |  1         -0.105210         -0.344567
     |  2         -0.306184          0.248063
     |
     |  Method resolution order:
     |      TextEncoder
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      sklearn.base.TransformerMixin
     |      sklearn.utils._set_output._SetOutputMixin
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __init__(self, model_name='intfloat/e5-small-v2', n_components=30, device=None, batch_size=32, token_env_variable=None, cache_folder=None, store_weights_in_pickle=False, random_state=None, verbose=False)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit_transform(self, column, y=None)
     |      Fit the TextEncoder from ``column``.
     |
     |      In practice, it loads the pre-trained model from disk and returns
     |      the embeddings of the column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series of shape (n_samples,)
     |          The string column to compute embeddings from.
     |
     |      y : None
     |          Unused. Here for compatibility with scikit-learn.
     |
     |      Returns
     |      -------
     |      X_out : pandas or polars DataFrame of shape (n_samples, n_components)
     |          The embedding representation of the input.
     |
     |  get_feature_names_out(self)
     |      Get output feature names for transformation.
     |
     |      Returns
     |      -------
     |      feature_names_out : list of str
     |          Transformed feature names.
     |
     |  set_fit_request(self: skrub._text_encoder.TextEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._text_encoder.TextEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  set_transform_request(self: skrub._text_encoder.TextEncoder, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._text_encoder.TextEncoder from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``transform`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``transform``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  transform(self, column)
     |      Transform ``column`` using the TextEncoder.
     |
     |      This method uses the embedding model loaded in memory during ``fit``
     |      or ``fit_transform``.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series of shape (n_samples,)
     |          The string column to compute embeddings from.
     |
     |      Returns
     |      -------
     |      X_out : pandas or polars DataFrame of shape (n_samples, n_components)
     |          The embedding representation of the input.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  fit(self, column, y=None)
     |      Fit the transformer.
     |
     |      Subclasses should implement ``fit_transform`` and ``transform``.
     |
     |      Parameters
     |      ----------
     |      column : a pandas or polars Series
     |          Unlike most scikit-learn transformers, single-column transformers
     |          transform a single column, not a whole dataframe.
     |
     |      y : column or dataframe
     |          Prediction targets.
     |
     |      Returns
     |      -------
     |      self
     |          The fitted transformer.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __init_subclass__(**kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  __sklearn_tags__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._set_output._SetOutputMixin:
     |
     |  set_output(self, *, transform=None)
     |      Set output container.
     |
     |      See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
     |      for an example on how to use the API.
     |
     |      Parameters
     |      ----------
     |      transform : {"default", "pandas", "polars"}, default=None
     |          Configure output of `transform` and `fit_transform`.
     |
     |          - `"default"`: Default output format of a transformer
     |          - `"pandas"`: DataFrame output
     |          - `"polars"`: Polars output
     |          - `None`: Transform configuration is unchanged
     |
     |          .. versionadded:: 1.4
     |              `"polars"` option was added.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.

    class ToCategorical(skrub._on_each_column.SingleColumnTransformer)
     |  Convert a string column to Categorical dtype.
     |
     |  .. note::
     |
     |      ``ToCategorical`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((ToCategorical(), 'col_name_1'),
     |      (ToCategorical(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((ToCategorical(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  The main benefit is that categorical columns can then be recognized by
     |  scikit-learn's ``HistGradientBoostingRegressor`` and
     |  ``HistGradientBoostingClassifier`` with their
     |  ``categorical_features='from_dtype'`` option. This transformer is therefore
     |  particularly useful as the ``low_cardinality_transformer`` parameter of the
     |  ``TableVectorizer`` when combined with one of those supervised learners.
     |
     |  A pandas column with dtype ``string`` or ``object`` containing strings, or
     |  a polars column with dtype ``String``, is converted to a categorical
     |  column. Categorical columns are passed through.
     |
     |  Any other type of column is rejected by raising a ``RejectColumn``
     |  exception. **Note:** the ``TableVectorizer`` only sends string or
     |  categorical columns to its ``low_cardinality_transformer``. Therefore it is
     |  always safe to use a ``ToCategorical`` instance as the
     |  ``low_cardinality_transformer``.
     |
     |  The output of ``transform`` also always has a Categorical dtype. The categories
     |  are not necessarily the same across different calls to ``transform``. Indeed,
     |  scikit-learn estimators do not inspect the dtype's categories but the actual
     |  values. Converting to a Categorical is therefore just a way to mark a
     |  column and indicate to downstream estimators that this column should be treated
     |  as categorical. Ensuring they are encoded consistently, handling unseen
     |  categories at test time, etc. is the responsibility of encoders such as
     |  ``OneHotEncoder`` and ``LabelEncoder``, or of estimators that handle categories
     |  themselves such as ``HistGradientBoostingRegressor``.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |  &gt;&gt;&gt; from skrub import ToCategorical
     |
     |  A string column is converted to a categorical column.
     |
     |  &gt;&gt;&gt; s = pd.Series(['one', 'two', None], name='c')
     |  &gt;&gt;&gt; s
     |  0     one
     |  1     two
     |  2    None
     |  Name: c, dtype: object
     |  &gt;&gt;&gt; to_cat = ToCategorical()
     |  &gt;&gt;&gt; to_cat.fit_transform(s)
     |  0    one
     |  1    two
     |  2    NaN
     |  Name: c, dtype: category
     |  Categories (2, object): ['one', 'two']
     |
     |  The dtypes (the list of categories) of the outputs of ``transform`` may
     |  vary. This transformer only ensures the dtype is Categorical to mark the
     |  column as such for downstream encoders which will perform the actual
     |  encoding.
     |
     |  &gt;&gt;&gt; s = pd.Series(['four', 'five'], name='c')
     |  &gt;&gt;&gt; to_cat.transform(s)
     |  0    four
     |  1    five
     |  Name: c, dtype: category
     |  Categories (2, object): ['five', 'four']
     |
     |  Columns that already have a Categorical dtype are passed through:
     |
     |  &gt;&gt;&gt; s = pd.Series(['one', 'two'], name='c', dtype='category')
     |  &gt;&gt;&gt; to_cat.fit_transform(s) is s
     |  True
     |
     |  Columns that are not strings nor categorical are rejected:
     |
     |  &gt;&gt;&gt; to_cat.fit_transform(pd.Series([1.1, 2.2], name='c'))
     |  Traceback (most recent call last):
     |      ...
     |  skrub._on_each_column.RejectColumn: Column 'c' does not contain strings.
     |
     |  ``object`` columns that do not contain only strings are also rejected:
     |
     |  &gt;&gt;&gt; s = pd.Series(['one', 1], name='c')
     |  &gt;&gt;&gt; to_cat.fit_transform(s)
     |  Traceback (most recent call last):
     |      ...
     |  skrub._on_each_column.RejectColumn: Column 'c' does not contain strings.
     |
     |  No special handling of ``StringDtype`` vs ``object`` columns is done, the
     |  behavior is the same as ``pd.astype('category')``: if the input uses the
     |  extension dtype, the categories of the output will, too.
     |
     |  &gt;&gt;&gt; s = pd.Series(['cat A', 'cat B', None], name='c', dtype='string')
     |  &gt;&gt;&gt; s
     |  0    cat A
     |  1    cat B
     |  2     &lt;NA&gt;
     |  Name: c, dtype: string
     |  &gt;&gt;&gt; to_cat.fit_transform(s)
     |  0    cat A
     |  1    cat B
     |  2     &lt;NA&gt;
     |  Name: c, dtype: category
     |  Categories (2, string): [cat A, cat B]
     |  &gt;&gt;&gt; _.cat.categories.dtype
     |  string[python]
     |
     |  Polars string columns are converted to the ``Categorical`` dtype (not ``Enum``). As
     |  for pandas, categories may vary across calls to ``transform``.
     |
     |  &gt;&gt;&gt; import pytest
     |  &gt;&gt;&gt; pl = pytest.importorskip("polars")
     |  &gt;&gt;&gt; s = pl.Series('c', ['one', 'two', None])
     |  &gt;&gt;&gt; to_cat.fit_transform(s)
     |  shape: (3,)
     |  Series: 'c' [cat]
     |  [
     |      "one"
     |      "two"
     |      null
     |  ]
     |
     |  Polars Categorical or Enum columns are passed through:
     |
     |  &gt;&gt;&gt; s = pl.Series('c', ['one', 'two'], dtype=pl.Enum(['one', 'two', 'three']))
     |  &gt;&gt;&gt; s
     |  shape: (2,)
     |  Series: 'c' [enum]
     |  [
     |      "one"
     |      "two"
     |  ]
     |  &gt;&gt;&gt; to_cat.fit_transform(s) is s
     |  True
     |
     |  Method resolution order:
     |      ToCategorical
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  fit_transform(self, column, y=None)
     |      Fit the encoder and transform a column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series
     |          The input to transform.
     |
     |      y : None
     |          Ignored.
     |
     |      Returns
     |      -------
     |      transformed : pandas or polars Series
     |          The input transformed to Categorical.
     |
     |  set_fit_request(self: skrub._to_categorical.ToCategorical, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._to_categorical.ToCategorical from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  set_transform_request(self: skrub._to_categorical.ToCategorical, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._to_categorical.ToCategorical from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``transform`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``transform``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  transform(self, column)
     |      Transform a column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series
     |          The input to transform.
     |
     |      Returns
     |      -------
     |      transformed : pandas or polars Series
     |          The input transformed to Categorical.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  fit(self, column, y=None)
     |      Fit the transformer.
     |
     |      Subclasses should implement ``fit_transform`` and ``transform``.
     |
     |      Parameters
     |      ----------
     |      column : a pandas or polars Series
     |          Unlike most scikit-learn transformers, single-column transformers
     |          transform a single column, not a whole dataframe.
     |
     |      y : column or dataframe
     |          Prediction targets.
     |
     |      Returns
     |      -------
     |      self
     |          The fitted transformer.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __init_subclass__(**kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  __sklearn_tags__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

    class ToDatetime(skrub._on_each_column.SingleColumnTransformer)
     |  ToDatetime(format=None)
     |
     |  Parse datetimes represented as strings and return ``Datetime`` columns.
     |
     |  .. note::
     |
     |      ``ToDatetime`` is a type of single-column transformer. Unlike
     |      most scikit-learn estimators, its ``fit``, ``transform`` and
     |      ``fit_transform`` methods expect a single column (a pandas or
     |      polars Series) rather than a full dataframe. To apply this
     |      transformer to one or more columns in a dataframe, use it as a
     |      parameter in a ``skrub.TableVectorizer`` or
     |      ``sklearn.compose.ColumnTransformer``. In the
     |      ``ColumnTransformer``, pass a single column:
     |      ``make_column_transformer((ToDatetime(), 'col_name_1'),
     |      (ToDatetime(), 'col_name_2'))`` instead of
     |      ``make_column_transformer((ToDatetime(), ['col_name_1',
     |      'col_name_2']))``.
     |
     |  An input column is converted to a column with dtype Datetime if possible,
     |  and rejected by raising a ``RejectColumn`` exception otherwise. Only Date,
     |  Datetime, String, and pandas object columns are handled, other dtypes are
     |  rejected with ``RejectColumn``.
     |
     |  Once a column is accepted, outputs of ``transform`` always have the same
     |  Datetime dtype (including resolution and time zone). Once the transformer
     |  is fitted, entries that fail to be converted during subsequent calls to
     |  ``transform`` are replaced with nulls.
     |
     |  Parameters
     |  ----------
     |  format : str or None, optional, default=None
     |      Format to use for parsing dates that are stored as strings, e.g.
     |      ``"%Y-%m-%dT%H:%M%S"``.
     |      If not specified, the format is inferred from the data when possible.
     |      When doing so, for dates presented as 01/02/2003, it is usually
     |      possible to infer from the data whether the month comes first (USA
     |      convention) or the day comes first, ie ``"%m/%d/%Y"`` vs
     |      ``"%d/%m/%Y"``. In the odd chance that all the sampled dates land
     |      before the 13th day of the month and that both conventions are
     |      plausible, the USA convention (month first) is chosen.
     |
     |  Attributes
     |  ----------
     |  format_ : str or None
     |      Detected format. If the transformer was fitted on a column that already had
     |      a Datetime dtype, the ``format_`` is None. Otherwise it is the
     |      format that was detected when parsing the string column. If the parameter
     |      ``format`` was provided, it is the only one that the transformer
     |      attempts to use so in that caset ``format_`` is either ``None`` or
     |      equal to ``format``.
     |
     |  output_dtype_ : data type
     |      The output dtype, which includes information about the time resolution and
     |      time zone.
     |
     |  output_time_zone_ : str or None
     |      The time zone of the transformed column. If the output is time zone naive it
     |      is ``None``; otherwise it is the name of the time zone such as ``UTC`` or
     |      ``Europe/Paris``.
     |
     |  Examples
     |  --------
     |  &gt;&gt;&gt; import pandas as pd
     |
     |  &gt;&gt;&gt; s = pd.Series(["2024-05-05T13:17:52", None, "2024-05-07T13:17:52"], name="when")
     |  &gt;&gt;&gt; s
     |  0    2024-05-05T13:17:52
     |  1                   None
     |  2    2024-05-07T13:17:52
     |  Name: when, dtype: object
     |
     |  &gt;&gt;&gt; from skrub._to_datetime import ToDatetime
     |
     |  &gt;&gt;&gt; to_dt = ToDatetime()
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  0   2024-05-05 13:17:52
     |  1                   NaT
     |  2   2024-05-07 13:17:52
     |  Name: when, dtype: datetime64[...]
     |
     |  The attributes ``format_``, ``output_dtype_``, ``output_time_zone_``
     |  record information about the conversion result.
     |
     |  &gt;&gt;&gt; to_dt.format_
     |  '%Y-%m-%dT%H:%M:%S'
     |  &gt;&gt;&gt; to_dt.output_dtype_
     |  dtype('&lt;M8[...]')
     |  &gt;&gt;&gt; to_dt.output_time_zone_ is None
     |  True
     |
     |  If we provide the datetime format, it is used and columns that do not conform to
     |  it are rejected.
     |
     |  &gt;&gt;&gt; ToDatetime(format="%Y-%m-%dT%H:%M:%S").fit_transform(s)
     |  0   2024-05-05 13:17:52
     |  1                   NaT
     |  2   2024-05-07 13:17:52
     |  Name: when, dtype: datetime64[...]
     |
     |  &gt;&gt;&gt; ToDatetime(format="%d/%m/%Y").fit_transform(s)
     |  Traceback (most recent call last):
     |      ...
     |  skrub._on_each_column.RejectColumn: Failed to convert column 'when' to datetimes using the format '%d/%m/%Y'.
     |
     |  Columns that already have ``Datetime`` ``dtype`` are not modified (but
     |  they are accepted); for those columns the provided format, if any, is ignored.
     |
     |  &gt;&gt;&gt; s = pd.to_datetime(s).dt.tz_localize("Europe/Paris")
     |  &gt;&gt;&gt; s
     |  0   2024-05-05 13:17:52+02:00
     |  1                         NaT
     |  2   2024-05-07 13:17:52+02:00
     |  Name: when, dtype: datetime64[..., Europe/Paris]
     |  &gt;&gt;&gt; to_dt.fit_transform(s) is s
     |  True
     |
     |  In that case the ``format_`` is ``None``.
     |
     |  &gt;&gt;&gt; to_dt.format_ is None
     |  True
     |  &gt;&gt;&gt; to_dt.output_dtype_
     |  datetime64[..., Europe/Paris]
     |  &gt;&gt;&gt; to_dt.output_time_zone_
     |  'Europe/Paris'
     |
     |  Columns that have a different ``dtype`` than strings, pandas objects, or
     |  datetimes are rejected.
     |
     |  &gt;&gt;&gt; s = pd.Series([2020, 2021, 2022], name="year")
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  Traceback (most recent call last):
     |      ...
     |  skrub._on_each_column.RejectColumn: Column 'year' does not contain strings.
     |
     |  String columns that do not appear to contain datetimes or for some other reason
     |  fail to be converted are also rejected.
     |
     |  &gt;&gt;&gt; s = pd.Series(["2024-05-07T13:36:27", "yesterday"], name="when")
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  Traceback (most recent call last):
     |      ...
     |  skrub._on_each_column.RejectColumn: Could not find a datetime format for column 'when'.
     |
     |  Once ``ToDatetime`` was successfully fitted, ``transform`` will always try to
     |  parse datetimes with the same format and output the same ``dtype``. Entries that
     |  fail to be converted result in a null value:
     |
     |  &gt;&gt;&gt; s = pd.Series(["2024-05-05T13:17:52", None, "2024-05-07T13:17:52"], name="when")
     |  &gt;&gt;&gt; to_dt = ToDatetime().fit(s)
     |  &gt;&gt;&gt; to_dt.transform(s)
     |  0   2024-05-05 13:17:52
     |  1                   NaT
     |  2   2024-05-07 13:17:52
     |  Name: when, dtype: datetime64[...]
     |  &gt;&gt;&gt; s = pd.Series(["05/05/2024", None, "07/05/2024"], name="when")
     |  &gt;&gt;&gt; to_dt.transform(s)
     |  0   NaT
     |  1   NaT
     |  2   NaT
     |  Name: when, dtype: datetime64[...]
     |
     |  **Time zones**
     |
     |  During ``fit``, parsing strings that contain fixed offsets results in datetimes
     |  in UTC. Mixed offsets are supported and will all be converted to UTC.
     |
     |  &gt;&gt;&gt; s = pd.Series(["2020-01-01T04:00:00+02:00", "2020-01-01T04:00:00+03:00"])
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  0   2020-01-01 02:00:00+00:00
     |  1   2020-01-01 01:00:00+00:00
     |  dtype: datetime64[..., UTC]
     |  &gt;&gt;&gt; to_dt.format_
     |  '%Y-%m-%dT%H:%M:%S%z'
     |  &gt;&gt;&gt; to_dt.output_time_zone_
     |  'UTC'
     |
     |  Strings with no timezone indication result in naive datetimes:
     |
     |  &gt;&gt;&gt; s = pd.Series(["2020-01-01T04:00:00", "2020-01-01T04:00:00"])
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  0   2020-01-01 04:00:00
     |  1   2020-01-01 04:00:00
     |  dtype: datetime64[...]
     |  &gt;&gt;&gt; to_dt.output_time_zone_ is None
     |  True
     |
     |  During ``transform``, outputs are cast to the same ``dtype`` that was found
     |  during ``fit``. This includes the timezone, which is converted if necessary.
     |
     |  &gt;&gt;&gt; s_paris = pd.to_datetime(
     |  ...     pd.Series(["2024-05-07T14:24:49", "2024-05-06T14:24:49"])
     |  ... ).dt.tz_localize("Europe/Paris")
     |  &gt;&gt;&gt; s_paris
     |  0   2024-05-07 14:24:49+02:00
     |  1   2024-05-06 14:24:49+02:00
     |  dtype: datetime64[..., Europe/Paris]
     |  &gt;&gt;&gt; to_dt = ToDatetime().fit(s_paris)
     |  &gt;&gt;&gt; to_dt.output_dtype_
     |  datetime64[..., Europe/Paris]
     |
     |  Here our converter is set to output datetimes with nanosecond resolution,
     |  localized in "Europe/Paris".
     |
     |  We may have a column in a different timezone:
     |
     |  &gt;&gt;&gt; s_london = s_paris.dt.tz_convert("Europe/London")
     |  &gt;&gt;&gt; s_london
     |  0   2024-05-07 13:24:49+01:00
     |  1   2024-05-06 13:24:49+01:00
     |  dtype: datetime64[..., Europe/London]
     |
     |  Here the timezone is "Europe/London" and the times are offset by 1 hour. During
     |  ``transform`` datetimes will be converted to the original dtype and the
     |  "Europe/Paris" timezone:
     |
     |  &gt;&gt;&gt; to_dt.transform(s_london)
     |  0   2024-05-07 14:24:49+02:00
     |  1   2024-05-06 14:24:49+02:00
     |  dtype: datetime64[..., Europe/Paris]
     |
     |  Moreover, we may have to transform a timezone-naive column whereas the
     |  transformer was fitted on a timezone-aware column. Note that is somewhat a
     |  corner case unlikely to happen in practice if the inputs to ``fit`` and
     |  ``transform`` come from the same dataframe.
     |
     |  &gt;&gt;&gt; s_naive = s_paris.dt.tz_convert(None)
     |  &gt;&gt;&gt; s_naive
     |  0   2024-05-07 12:24:49
     |  1   2024-05-06 12:24:49
     |  dtype: datetime64[...]
     |
     |  In this case, we make the arbitrary choice to assume that the timezone-naive
     |  datetimes are in UTC.
     |
     |  &gt;&gt;&gt; to_dt.transform(s_naive)
     |  0   2024-05-07 14:24:49+02:00
     |  1   2024-05-06 14:24:49+02:00
     |  dtype: datetime64[..., Europe/Paris]
     |
     |  Conversely, a transformer fitted on a timezone-naive column can convert
     |  timezone-aware columns. Here also, we assume the naive datetimes were in UTC.
     |
     |  &gt;&gt;&gt; to_dt = ToDatetime().fit(s_naive)
     |  &gt;&gt;&gt; to_dt.transform(s_london)
     |  0   2024-05-07 12:24:49
     |  1   2024-05-06 12:24:49
     |  dtype: datetime64[...]
     |
     |  **``%d/%m/%Y`` vs ``%m/%d/%Y``**
     |
     |  When parsing strings in one of the formats above, ``ToDatetime`` tries to guess
     |  if the month comes first (USA convention) or the day (rest of the world) from
     |  the data.
     |
     |  &gt;&gt;&gt; s = pd.Series(["05/23/2024"])
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  0   2024-05-23
     |  dtype: datetime64[...]
     |  &gt;&gt;&gt; to_dt.format_
     |  '%m/%d/%Y'
     |
     |  Here we could infer ``'%m/%d/%Y'`` because there are not 23 months in a year.
     |  Similarly,
     |
     |  &gt;&gt;&gt; s = pd.Series(["23/05/2024"])
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  0   2024-05-23
     |  dtype: datetime64[...]
     |  &gt;&gt;&gt; to_dt.format_
     |  '%d/%m/%Y'
     |
     |  In the case it cannot be inferred, the USA convention is used:
     |
     |  &gt;&gt;&gt; s = pd.Series(["03/05/2024"])
     |  &gt;&gt;&gt; to_dt.fit_transform(s)
     |  0   2024-03-05
     |  dtype: datetime64[...]
     |  &gt;&gt;&gt; to_dt.format_
     |  '%m/%d/%Y'
     |
     |  If the days are randomly distributed and the fitting data large enough, it is
     |  somewhat unlikely that all days would be below 12 so the inferred format should
     |  often be correct. To be sure, one can specify the ``format`` in the
     |  constructor.
     |
     |  Method resolution order:
     |      ToDatetime
     |      skrub._on_each_column.SingleColumnTransformer
     |      sklearn.base.BaseEstimator
     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin
     |      sklearn.utils._metadata_requests._MetadataRequester
     |      builtins.object
     |
     |  Methods defined here:
     |
     |  __init__(self, format=None)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |
     |  fit_transform(self, column, y=None)
     |      Fit the encoder and transform a column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series
     |          The input to transform.
     |
     |      y : None
     |          Ignored.
     |
     |      Returns
     |      -------
     |      transformed : pandas or polars Series.
     |          The input transformed to Datetime.
     |
     |  set_fit_request(self: skrub._to_datetime.ToDatetime, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._to_datetime.ToDatetime from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``fit`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``fit``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  set_transform_request(self: skrub._to_datetime.ToDatetime, *, column: Union[bool, NoneType, str] = '$UNCHANGED$') -&gt; skrub._to_datetime.ToDatetime from sklearn.utils._metadata_requests.RequestMethod.__get__.&lt;locals&gt;
     |      Request metadata passed to the ``transform`` method.
     |
     |      Note that this method is only relevant if
     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).
     |      Please see :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      The options for each parameter are:
     |
     |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.
     |
     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.
     |
     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.
     |
     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.
     |
     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the
     |      existing request. This allows you to change the request for some
     |      parameters and not others.
     |
     |      .. versionadded:: 1.3
     |
     |      .. note::
     |          This method is only relevant if this estimator is used as a
     |          sub-estimator of a meta-estimator, e.g. used inside a
     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.
     |
     |      Parameters
     |      ----------
     |      column : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED
     |          Metadata routing for ``column`` parameter in ``transform``.
     |
     |      Returns
     |      -------
     |      self : object
     |          The updated object.
     |
     |  transform(self, column)
     |      Transform a column.
     |
     |      Parameters
     |      ----------
     |      column : pandas or polars Series
     |          The input to transform.
     |
     |      Returns
     |      -------
     |      transformed : pandas or polars Series.
     |          The input transformed to Datetime.
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |
     |  __annotations__ = {}
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  fit(self, column, y=None)
     |      Fit the transformer.
     |
     |      Subclasses should implement ``fit_transform`` and ``transform``.
     |
     |      Parameters
     |      ----------
     |      column : a pandas or polars Series
     |          Unlike most scikit-learn transformers, single-column transformers
     |          transform a single column, not a whole dataframe.
     |
     |      y : column or dataframe
     |          Prediction targets.
     |
     |      Returns
     |      -------
     |      self
     |          The fitted transformer.
     |
     |  ----------------------------------------------------------------------
     |  Class methods inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __init_subclass__(**kwargs)
     |      Set the ``set_{method}_request`` methods.
     |
     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It
     |      looks for the information available in the set default values which are
     |      set using ``__metadata_request__*`` class attributes, or inferred
     |      from method signatures.
     |
     |      The ``__metadata_request__*`` class attributes are used when a method
     |      does not explicitly accept a metadata through its arguments or if the
     |      developer would like to specify a request value for those metadata
     |      which are different from the default ``None``.
     |
     |      References
     |      ----------
     |      .. [1] https://www.python.org/dev/peps/pep-0487
     |
     |  ----------------------------------------------------------------------
     |  Data and other attributes inherited from skrub._on_each_column.SingleColumnTransformer:
     |
     |  __single_column_transformer__ = True
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |
     |  __getstate__(self)
     |      Helper for pickle.
     |
     |  __repr__(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |
     |  __setstate__(self, state)
     |
     |  __sklearn_clone__(self)
     |
     |  __sklearn_tags__(self)
     |
     |  get_params(self, deep=True)
     |      Get parameters for this estimator.
     |
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |
     |  set_params(self, **params)
     |      Set the parameters of this estimator.
     |
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
     |      possible to update each component of a nested object.
     |
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
     |
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin:
     |
     |  __dict__
     |      dictionary for instance variables
     |
     |  __weakref__
     |      list of weak references to the object
     |
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:
     |
     |  get_metadata_routing(self)
     |      Get metadata routing of this object.
     |
     |      Please check :ref:`User Guide &lt;metadata_routing&gt;` on how the routing
     |      mechanism works.
     |
     |      Returns
     |      -------
     |      routing : MetadataRequest
     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating
     |          routing information.

FUNCTIONS
    column_associations(df)
        Get measures of statistical associations between all pairs of columns.

        At the moment, the only reported metric is Cramer's V statistic. More may
        be added in the future.

        The result is returned as a dataframe with columns:

        ``['left_column_name', 'left_column_idx', 'right_column_name',
        'right_column_idx', 'cramer_v']``

        As the function is commutative, each pair of columns appears only once
        (either ``col_1``, ``col_2`` or ``col_2``, ``col_1`` but not both).
        The results are sorted
        from most associated to least associated.

        To compute the Cramer's V statistic, all columns are discretized. Numeric
        columns are binned with 10 bins. For categorical columns, only the 10 most
        frequent categories are considered. In both cases, nulls are treated as a
        separate category, ie a separate row in the contingency table. Thus
        associations between the values of 2 columns or between their missingness
        patterns may be captured.

        Parameters
        ----------
        df : dataframe
            The dataframe whose columns will be compared to each other.

        Returns
        -------
        dataframe
            The computed associations.

        Notes
        -----
        Cramér's V is a measure of association between two nominal variables,
        giving a value between 0 and +1 (inclusive).

        * `Cramer's V &lt;https://en.wikipedia.org/wiki/Cramér%27s_V&gt;`_

        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; import pandas as pd
        &gt;&gt;&gt; import skrub
        &gt;&gt;&gt; pd.set_option('display.width', 200)
        &gt;&gt;&gt; pd.set_option('display.max_columns', 10)
        &gt;&gt;&gt; pd.set_option('display.precision', 4)
        &gt;&gt;&gt; rng = np.random.default_rng(33)
        &gt;&gt;&gt; df = pd.DataFrame({f"c_{i}": rng.random(size=20)*10 for i in range(5)})
        &gt;&gt;&gt; df["c_str"] = [f"val {i}" for i in range(df.shape[0])]
        &gt;&gt;&gt; df.shape
        (20, 6)
        &gt;&gt;&gt; df.head()
              c_0     c_1     c_2     c_3     c_4  c_str
        0  4.4364  4.0114  6.9271  7.0970  4.8913  val 0
        1  5.6849  0.7192  7.6430  4.6441  2.5116  val 1
        2  9.0810  9.4011  1.9257  5.7429  6.2358  val 2
        3  2.5425  2.9678  9.7801  9.9879  6.0709  val 3
        4  5.8878  9.3223  5.3840  7.2006  2.1494  val 4
        &gt;&gt;&gt; associations = skrub.column_associations(df)
        &gt;&gt;&gt; associations # doctest: +SKIP
           left_column_name  left_column_idx right_column_name  right_column_idx  cramer_v
        0               c_3                3             c_str                 5    0.8215
        1               c_1                1               c_4                 4    0.8215
        2               c_0                0               c_1                 1    0.8215
        3               c_2                2             c_str                 5    0.7551
        4               c_0                0             c_str                 5    0.7551
        5               c_0                0               c_3                 3    0.7551
        6               c_1                1               c_3                 3    0.6837
        7               c_0                0               c_4                 4    0.6837
        8               c_4                4             c_str                 5    0.6837
        9               c_3                3               c_4                 4    0.6053
        10              c_2                2               c_3                 3    0.6053
        11              c_1                1             c_str                 5    0.6053
        12              c_0                0               c_2                 2    0.6053
        13              c_2                2               c_4                 4    0.5169
        14              c_1                1               c_2                 2    0.4122
        &gt;&gt;&gt; pd.reset_option('display.width')
        &gt;&gt;&gt; pd.reset_option('display.max_columns')
        &gt;&gt;&gt; pd.reset_option('display.precision')

    compute_ngram_distance(unique_words, ngram_range=(2, 4), analyzer='char_wb')
        Compute the condensed pair-wise n-gram distance between `unique_words`.

        Parameters
        ----------
        unique_words : sequence of str
            Sequence or array of unique words from the original data.
        ngram_range : 2-tuple of int, default=(2,4)
            The lower and upper boundaries of the range of n-values for different
            n-grams used in the string similarity. All values of `n` such
            that ``min_n &lt;= n &lt;= max_n`` will be used.
        analyzer : str, default='char_wb'
            Analyzer to extract n-grams.

        Returns
        -------
        ndarray
            An n-times-(n-1)/2 array of n-gram tf-idf distances between `unique_words`.

        Notes
        -----
        Extracts n-grams of all elements in `unique_words`, calculates the
        term frequency-inverse document frequency (TF-IDF) for each n-gram, then
        computes the pair-wise Euclidean distance between elements based on their
        n-gram TF-IDF representation.

    deduplicate(X, *, n_clusters=None, ngram_range=(2, 4), analyzer='char_wb', linkage_method='average', n_jobs=None)
        Deduplicate categorical data by hierarchically clustering similar strings.

        This works best if there are a number of underlying categories that
        sometimes appear in the data with small variations and/or misspellings.

        Parameters
        ----------
        X : sequence of str
            The data to be deduplicated.
        n_clusters : int, default=None
            Number of clusters to use for hierarchical clustering, if `None` use the
            number of clusters that lead to the lowest silhouette score.
        ngram_range : 2-tuple of int, default=(2, 4)
            The lower and upper boundaries of the range of n-values for different
            n-grams used in the string similarity. All values of `n` such
            that ``min_n &lt;= n &lt;= max_n`` will be used.
        analyzer : {'word', 'char', 'char_wb'}, default='char_wb'
            Analyzer parameter for the CountVectorizer
            used for the string similarities.
            Describes whether the matrix `V` to factorize should be made of
            word counts or character n-gram counts.
            Option `char_wb` creates character n-grams only from text inside word
            boundaries; n-grams at the edges of words are padded with space.
        linkage_method : {'single', 'complete', 'average', 'centroid', 'median', 'ward'},
            default='average'
            Linkage method parameter to use for merging clusters via
            :func:`scipy.cluster.hierarchy.linkage`.
            Option 'average' calculates the distance between two clusters as the
            average distance between data points in the first and second cluster.
        n_jobs : int, default=None
            The number of jobs to run in parallel.

        Returns
        -------
        list of str
           The deduplicated data.

        See Also
        --------
        GapEncoder :
            Encodes dirty categories (strings) by constructing latent topics with
            continuous encoding.
        MinHashEncoder :
            Encode string columns as a numeric array with the minhash method.
        SimilarityEncoder :
            Encode string columns as a numeric array with n-gram string similarity.

        Notes
        -----
        Deduplication is done by first computing the n-gram distance between unique
        categories in data, then performing hierarchical clustering on this distance
        matrix, and choosing the most frequent element in each cluster as the
        'correct' spelling. This method works best if the true number of
        categories is significantly smaller than the number of observed spellings.

        Examples
        --------
        &gt;&gt;&gt; from skrub.datasets import make_deduplication_data
        &gt;&gt;&gt; duplicated = make_deduplication_data(examples=['black', 'white'],
        ...                                      entries_per_example=[5, 5],
        ...                                      prob_mistake_per_letter=0.3,
        ...                                      random_state=42)
        &gt;&gt;&gt; duplicated
        ['blacs', 'black', 'black', 'black', 'black', 'uhibe', 'white', 'white', 'white', 'white']

        To deduplicate the data, we can build a correspondence matrix:

        &gt;&gt;&gt; from skrub import deduplicate
        &gt;&gt;&gt; deduplicate_correspondence = deduplicate(duplicated)
        &gt;&gt;&gt; deduplicate_correspondence
        blacs    black
        black    black
        black    black
        black    black
        black    black
        uhibe    white
        white    white
        white    white
        white    white
        white    white
        dtype: object

        The translation table above is actually a series, giving the deduplicated values,
        and indexed by the original values.
        A deduplicated version of the initial list can easily be created:

        &gt;&gt;&gt; deduplicated = list(deduplicate_correspondence)
        &gt;&gt;&gt; deduplicated
        ['black', 'black', 'black', 'black', 'black', 'white', 'white', 'white', 'white', 'white']

    fuzzy_join(left, right, left_on=None, right_on=None, on=None, suffix='', max_dist=inf, ref_dist='random_pairs', string_encoder=Pipeline(steps=[('functiontransformer',
                     FunctionTransformer(func=functools.partial(&lt;function fill_nulls at 0x0000012A0D6F37E0&gt;, value=''))),
                    ('tostr', ToStr()),
                    ('hashingvectorizer',
                     HashingVectorizer(analyzer='char_wb', ngram_range=(2, 4))),
                    ('tfidftransformer', TfidfTransformer())]), add_match_info=False, drop_unmatched=False)
        Fuzzy (approximate) join.

        Rows in the left table are joined to their closest match from the right
        table. The resulting table has the same rows (in the same order) as the
        left table, unless ``drop_unmatched`` is ``True``, in which case rows that
        are too far from their closest match will not appear in the result. Each
        row from the left table appears at most once in the result; if there are
        several equally good matching rows in the right table one of them will be
        used; which one is unspecified.

        To identify the best match for each row, values from the matching columns
        (``left_key`` and ``right_key``) are vectorized, i.e. represented by vectors of
        continuous values. Then, the Euclidean distances between these vectors are
        computed to find, for each left table row, its nearest neighbor within the
        right table.

        Optionally, a maximum distance threshold, ``max_dist``, can be set. Matches
        between vectors that are separated by a distance (strictly) greater than
        ``max_dist`` will be rejected. We will consider that left table rows that
        are farther than ``max_dist`` from their nearest neighbor do not have a
        matching row in the right table, and the output will contain nulls for
        the entries that would normally have come from the right table (as in a
        traditional left join).

        To make it easier to set a ``max_dist`` threshold, the distances are
        rescaled by dividing them by a reference distance, which can be chosen with
        ``ref_dist``. The default is ``'random_pairs'``. The possible choices are:

        'random_pairs'
            Pairs of rows are sampled randomly from the right table and their
            distance is computed. The reference distance is the first quartile of
            those distances.

        'second_neighbor'
            The reference distance is the distance to the *second* nearest neighbor
            in the right table.

        'self_join_neighbor'
            Once the match candidate (i.e. the nearest neighbor from the right
            table) has been found, we find its nearest neighbor in the right
            table (excluding itself). The reference distance is the distance that
            separates those 2 right rows.

        'no_rescaling'
            The reference distance is 1.0, i.e. no rescaling of the distances is
            applied.

        Parameters
        ----------
        left : :obj:`~pandas.DataFrame`
            Left operand of the join.
        right : :obj:`~pandas.DataFrame`
            Right operand of the join.
        left_on : str or list of str, default=None
            The column names in the left table on which the join will be performed.
            Can be a string if joining on a single column.
            If ``None``, `right_on` must also be ``None`` and `on` must be provided.
        right_on : str or list of str, default=None
            The column names in the right table on which the join will
            be performed. Can be a string if joining on a single column.
            If ``None``, `left_on` must also be ``None`` and `on` must be provided.
        on : str or list of str, default=None
            The column names to use for both ``left_on`` and ``right_on`` when they
            are the same. Provide either ``on`` or both ``left_on`` and ``right_on``.
        suffix : str, default=""
            Suffix to append to the ``right`` table's column names. You can use it
            to avoid duplicate column names in the join.
        max_dist : float, default=np.inf
            Maximum acceptable (rescaled) distance between a row in the
            ``left`` table and its nearest neighbor in the ``right`` table. Rows that
            are farther apart are not considered to match. By default, the distance
            is rescaled so that a value between 0 and 1 is typically a good choice,
            although rescaled distances can be greater than 1 for some choices of
            ``ref_dist``. ``None``, ``"inf"``, ``float("inf")`` or ``numpy.inf``
            mean that no matches are rejected.
        ref_dist : reference distance for rescaling, default = 'random_pairs'
            Options are {"random_pairs", "second_neighbor", "self_join_neighbor",
            "no_rescaling"}. See above for a description of each option. To
            facilitate the choice of ``max_dist``, distances between rows in
            ``left`` table and their nearest neighbor in ``right`` table will be
            rescaled by this reference distance.
        string_encoder : scikit-learn transformer used to vectorize text columns
            By default a ``HashingVectorizer`` combined with a ``TfidfTransformer``
            is used. Here we use raw TF-IDF features rather than transforming them
            for example with ``GapEncoder`` or ``MinHashEncoder`` because it is
            faster, these features are only used to find nearest neighbors and not
            used by downstream estimators, and distances between TF-IDF vectors
            have a somewhat simpler interpretation.
        add_match_info : bool, default=False
            Insert columns whose names start with `skrub_Joiner` containing
            the distance, rescaled distance and whether the rescaled distance is
            above the threshold. Those values can be helpful for an estimator that
            uses the joined features, or to inspect the result of the join and set
            a ``max_dist`` threshold.
        drop_unmatched : bool, default=False
            Remove rows for which a match was not found in the right table (i.e. for
            which the nearest neighbor is further than `max_dist`).

        Returns
        -------
        :obj:`~pandas.DataFrame`
            The joined tables.

        See Also
        --------
        Joiner :
            Same as fuzzy_join but as a scikit-learn transformer.

        Examples
        --------
        &gt;&gt;&gt; import pandas as pd
        &gt;&gt;&gt; from skrub import fuzzy_join
        &gt;&gt;&gt; left_table = pd.DataFrame({"Country": ["France", "Italia", "Georgia"]})
        &gt;&gt;&gt; right_table = pd.DataFrame( {"Country": ["Germany", "France", "Italy"],
        ...                            "Capital": ["Berlin", "Paris", "Rome"]} )
        &gt;&gt;&gt; left_table
          Country
        0  France
        1  Italia
        2  Georgia
        &gt;&gt;&gt; right_table
           Country Capital
        0  Germany  Berlin
        1   France   Paris
        2    Italy    Rome
        &gt;&gt;&gt; fuzzy_join(
        ...     left_table,
        ...     right_table,
        ...     on="Country",
        ...     suffix="_right",
        ...     max_dist=0.8,
        ...     add_match_info=False,
        ... )
          Country    Country_right    Capital_right
        0  France           France            Paris
        1  Italia            Italy             Rome
        2   Georgia              NaN              NaN
        &gt;&gt;&gt; fuzzy_join(
        ...     left_table,
        ...     right_table,
        ...     on="Country",
        ...     suffix="_right",
        ...     drop_unmatched=True,
        ...     max_dist=0.8,
        ...     add_match_info=False,
        ... )
          Country    Country_right    Capital_right
        0  France           France            Paris
        1  Italia            Italy             Rome
        &gt;&gt;&gt; fuzzy_join(
        ...     left_table,
        ...     right_table,
        ...     on="Country",
        ...     suffix="_right",
        ...     max_dist=float("inf"),
        ...     add_match_info=False,
        ... )
          Country    Country_right    Capital_right
        0  France           France            Paris
        1  Italia           Italy             Rome
        2  Georgia          Germany           Berlin

    patch_display(pandas=True, polars=True, verbose=1)
        Replace the default DataFrame HTML displays with ``skrub.TableReport``.

        This function replaces the HTML displays (what is shown when an object is
        the output of a jupyter notebook cell) of pandas and polars DataFrames
        with a TableReport.

        It can be undone with ``skrub.unpatch_display()``.

        Parameters
        ----------
        pandas : bool, optional (default=True)
            If False, do not override the displays for pandas dataframes.
        polars : bool, optional (default=True)
            If False, do not override the displays for polars dataframes.
        verbose : int, default = 1
            Whether to print progress information while table report is being generated.

            * verbose = 1 prints how many columns have been processed so far.
            * verbose = 0 silences the output.

        See Also
        --------
        unpatch_display :
            Undo the change made by this function.

        TableReport :
            Directly create a report from a dataframe.

    tabular_learner(estimator, *, n_jobs=None)
        Get a simple machine-learning pipeline for tabular data.

        Given a scikit-learn ``estimator``, this function creates a
        machine-learning pipeline that preprocesses tabular data to extract numeric
        features, impute missing values and scale the data if necessary, then applies the
        ``estimator``.

        Instead of an actual estimator, ``estimator`` can also be the special-cased strings
        ``'regressor'``, ``'regression'``, ``'classifier'``, ``'classification'`` to use a
        :obj:`~sklearn.ensemble.HistGradientBoostingRegressor` or a
        :obj:`~sklearn.ensemble.HistGradientBoostingClassifier` with default
        parameters.

        ``tabular_learner`` returns a scikit-learn :obj:`~sklearn.pipeline.Pipeline`
        with several steps:

        - A :obj:`TableVectorizer` transforms the tabular data into numeric
          features. Its parameters are chosen depending on the provided
          ``estimator``.
        - An optional :obj:`~sklearn.impute.SimpleImputer` imputes missing values
          by their mean and adds binary columns that indicate which values were
          missing. This step is only added if the ``estimator`` cannot handle
          missing values itself.
        - An optional :obj:`~sklearn.preprocessing.StandardScaler` centers and
          rescales the data. This step is not added (because it is unnecessary) when
          the ``estimator`` is a tree ensemble such as random forest or gradient
          boosting.
        - The last step is the provided ``estimator``.

        Read more in the :ref:`User Guide &lt;table_vectorizer&gt;`.

        .. note::
           ``tabular_learner`` is a recent addition and the heuristics used
           to define an appropriate preprocessing based on the ``estimator`` may change
           in future releases.

        Parameters
        ----------
        estimator : {"regressor", "regression", "classifier", "classification"} or scikit-learn estimator
            The estimator to use as the final step in the pipeline. Based on the type of
            estimator, the previous preprocessing steps and their respective parameters are
            chosen. The possible values are:

            - ``'regressor'`` or ``'regression'``: a :obj:`~sklearn.ensemble.HistGradientBoostingRegressor`
              is used as the final step;
            - ``'classifier'`` or ``'classification'``: a :obj:`~sklearn.ensemble.HistGradientBoostingClassifier`
              is used as the final step;
            - a scikit-learn estimator: the provided estimator is used as the final step.

        n_jobs : int, default=None
            Number of jobs to run in parallel in the :obj:`TableVectorizer` step.
            ``None`` means 1 unless in a joblib ``parallel_backend`` context.
            ``-1`` means using all processors.

        Returns
        -------
        Pipeline
            A scikit-learn :obj:`~sklearn.pipeline.Pipeline` chaining some
            preprocessing and the provided ``estimator``.

        Notes
        -----
        The parameter values for the :obj:`TableVectorizer` might differ depending on the
        version of scikit-learn:

        - support for categorical features in
          :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
          :class:`~sklearn.ensemble.HistGradientBoostingRegressor` was added in
          scikit-learn 1.4. Therefore, before this version, a
          :class:`~sklearn.preprocessing.OrdinalEncoder` is used for low-cardinality
          features.
        - support for missing values in
          :class:`~sklearn.ensemble.RandomForestClassifier` and
          :class:`~sklearn.ensemble.RandomForestRegressor` was added in scikit-learn 1.4.
          Therefore, before this version, a :class:`~sklearn.impute.SimpleImputer` is used
          to impute missing values.

        Examples
        --------
        &gt;&gt;&gt; from skrub import tabular_learner

        We can easily get a default pipeline for regression or classification:

        &gt;&gt;&gt; tabular_learner('regression')                                    # doctest: +SKIP
        Pipeline(steps=[('tablevectorizer',
                         TableVectorizer(high_cardinality=MinHashEncoder(),
                                         low_cardinality=ToCategorical())),
                        ('histgradientboostingregressor',
                         HistGradientBoostingRegressor(categorical_features='from_dtype'))])

        When requesting a ``'regression'``, the last step of the pipeline is set to a
        :obj:`~sklearn.ensemble.HistGradientBoostingRegressor`.

        &gt;&gt;&gt; tabular_learner('classification')                                   # doctest: +SKIP
        Pipeline(steps=[('tablevectorizer',
                         TableVectorizer(high_cardinality=MinHashEncoder(),
                                         low_cardinality=ToCategorical())),
                        ('histgradientboostingclassifier',
                         HistGradientBoostingClassifier(categorical_features='from_dtype'))])

        When requesting a ``'classification'``, the last step of the pipeline is set to a
        :obj:`~sklearn.ensemble.HistGradientBoostingClassifier`.

        This pipeline can be applied to rich tabular data:

        &gt;&gt;&gt; import pandas as pd
        &gt;&gt;&gt; X = pd.DataFrame(
        ...     {
        ...         "last_visit": ["2020-01-02", "2021-04-01", "2024-12-05", "2023-08-10"],
        ...         "medication": [None, "metformin", "paracetamol", "gliclazide"],
        ...         "insulin_prescriptions": ["N/A", 13, 0, 17],
        ...         "fasting_glucose": [35, 140, 44, 137],
        ...     }
        ... )
        &gt;&gt;&gt; y = [0, 1, 0, 1]
        &gt;&gt;&gt; X
           last_visit   medication insulin_prescriptions  fasting_glucose
        0  2020-01-02         None                   N/A               35
        1  2021-04-01    metformin                    13              140
        2  2024-12-05  paracetamol                     0               44
        3  2023-08-10   gliclazide                    17              137

        &gt;&gt;&gt; model = tabular_learner('classifier').fit(X, y)
        &gt;&gt;&gt; model.predict(X)
        array([0, 0, 0, 0])

        Rather than using the default estimator, we can provide our own scikit-learn
        estimator:

        &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression
        &gt;&gt;&gt; model = tabular_learner(LogisticRegression())
        &gt;&gt;&gt; model.fit(X, y)
        Pipeline(steps=[('tablevectorizer', TableVectorizer()),
                        ('simpleimputer', SimpleImputer(add_indicator=True)),
                        ('standardscaler', StandardScaler()),
                        ('logisticregression', LogisticRegression())])

        By applying only the first pipeline step we can see the transformed data that is
        sent to the supervised estimator (see the :obj:`TableVectorizer` documentation for
        details):

        &gt;&gt;&gt; model.named_steps['tablevectorizer'].transform(X)               # doctest: +SKIP
           last_visit_year  last_visit_month  ...  insulin_prescriptions  fasting_glucose
        0           2020.0               1.0  ...                    NaN             35.0
        1           2021.0               4.0  ...                   13.0            140.0
        2           2024.0              12.0  ...                    0.0             44.0
        3           2023.0               8.0  ...                   17.0            137.0

        The parameters of the :obj:`TableVectorizer` depend on the provided ``estimator``.

        &gt;&gt;&gt; tabular_learner(LogisticRegression())
        Pipeline(steps=[('tablevectorizer', TableVectorizer()),
                        ('simpleimputer', SimpleImputer(add_indicator=True)),
                        ('standardscaler', StandardScaler()),
                        ('logisticregression', LogisticRegression())])

        We see that for the :obj:`~sklearn.linear_model.LogisticRegression` we get
        the default configuration of the :obj:`TableVectorizer` which is intended
        to work well for a wide variety of downstream estimators. Moreover, as the
        :obj:`~sklearn.linear_model.LogisticRegression` cannot handle missing
        values, an imputation step is added. Finally, as many models require the
        inputs to be centered and on the same scale, centering and standard scaling
        is added.

        On the other hand, For the :obj:`~sklearn.ensemble.HistGradientBoostingClassifier`
        (generated with the string ``"classifier"``):

        &gt;&gt;&gt; tabular_learner('classifier')                                   # doctest: +SKIP
        Pipeline(steps=[('tablevectorizer',
                         TableVectorizer(high_cardinality=MinHashEncoder(),
                                         low_cardinality=ToCategorical())),
                        ('histgradientboostingclassifier',
                         HistGradientBoostingClassifier(categorical_features='from_dtype'))])

        - A :obj:`MinHashEncoder` is used as the
          ``high_cardinality``. This encoder provides good
          performance when the supervised estimator is based on a decision tree
          or ensemble of trees, as is the case for the
          :obj:`~sklearn.ensemble.HistGradientBoostingClassifier`. Unlike the
          default :obj:`GapEncoder`, the :obj:`MinHashEncoder` does not produce
          interpretable features. However, it is much faster and uses less
          memory.

        - The ``low_cardinality`` does not one-hot encode features.
          The :obj:`~sklearn.ensemble.HistGradientBoostingClassifier` has built-in
          support for categorical data which is more efficient than one-hot
          encoding. Therefore the selected encoder, :obj:`ToCategorical`, simply
          makes sure that those features have a categorical dtype so that the
          :obj:`~sklearn.ensemble.HistGradientBoostingClassifier` recognizes them
          as such.

        - There is no missing-value imputation because the classifier has its own
          (better) mechanism for dealing with missing values.

        - There is no standard scaling which is unnecessary for trees and ensembles
          of trees.

    to_datetime(data, format=None)
        Convert DataFrame or column to Datetime dtype.

        Parameters
        ----------
        data : dataframe or column
            The dataframe or column to transform.

        format : str or None, optional, default=None
            Format string to use to parse datetime strings.
            See the reference documentation for format codes:
            https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes .

        Examples
        --------
        &gt;&gt;&gt; import pandas as pd
        &gt;&gt;&gt; from skrub import to_datetime
        &gt;&gt;&gt; X = pd.DataFrame(dict(a=[1, 2], b=["01/02/2021", "21/02/2021"]))
        &gt;&gt;&gt; X
           a           b
        0  1  01/02/2021
        1  2  21/02/2021
        &gt;&gt;&gt; to_datetime(X)
           a          b
        0  1 2021-02-01
        1  2 2021-02-21

    unpatch_display(pandas=True, polars=True)
        Undo the effect of ``skrub.patch_display()``.

        This function restores the default HTML displays of pandas and polars
        DataFrames.

        Parameters
        ----------
        pandas : bool, optional (default=True)
            If False, do not restore the displays for pandas dataframes.
        polars : bool, optional (default=True)
            If False, do not restore the displays for polars dataframes.

        See Also
        --------
        patch_display :
            Replace the default dataframe display with a TableReport.

        TableReport :
            Directly create a report from a dataframe.

DATA
    __all__ = ['TableReport', 'patch_display', 'unpatch_display', 'tabular...

VERSION
    0.5.1

FILE
    c:\users\nihar\appdata\local\programs\python\python312\lib\site-packages\skrub\__init__.py

</code></pre>
</div>
</div>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/24110043\.github\.io\/Blog-Website\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>